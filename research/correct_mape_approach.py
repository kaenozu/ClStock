#!/usr/bin/env python3
"""
æ­£ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§MAPE 10-20%ã‚’é”æˆã™ã‚‹äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ 
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_absolute_percentage_error
import logging
from utils.logger_config import setup_logger
from typing import Dict, List, Tuple
import warnings

warnings.filterwarnings("ignore")

from data.stock_data import StockDataProvider

# ãƒ­ã‚°è¨­å®š
logger = setup_logger(__name__)


class CorrectMAPEPredictor:
    """æ­£ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§MAPE 10-20%é”æˆã‚’ç›®æŒ‡ã™ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        self.data_provider = StockDataProvider()
        self.models = {}
        self.scalers = {}
        self.is_trained = False

    def create_comprehensive_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """åŒ…æ‹¬çš„ç‰¹å¾´é‡ä½œæˆï¼ˆChatGPTæ¨å¥¨ãƒ¬ãƒ™ãƒ«ï¼‰"""
        features = pd.DataFrame(index=data.index)

        # åŸºæœ¬ä¾¡æ ¼ç‰¹å¾´é‡
        features["price"] = data["Close"]
        features["volume"] = data["Volume"]
        features["high_low_ratio"] = data["High"] / data["Low"]
        features["price_volume"] = data["Close"] * data["Volume"]

        # è¤‡æ•°æœŸé–“ã®ãƒªã‚¿ãƒ¼ãƒ³ç‡
        for period in [1, 2, 3, 5, 7, 10, 15, 20]:
            features[f"return_{period}d"] = data["Close"].pct_change(period)

        # ç§»å‹•å¹³å‡ã¨ãã®é–¢ä¿‚
        for window in [5, 10, 20, 50, 100]:
            sma = data["Close"].rolling(window).mean()
            features[f"sma_{window}"] = sma
            features[f"price_sma_{window}_ratio"] = data["Close"] / sma
            features[f"sma_{window}_slope"] = sma.diff(5) / sma.shift(5)

        # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ç‰¹å¾´é‡
        for window in [5, 10, 20]:
            vol = data["Close"].pct_change().rolling(window).std()
            features[f"volatility_{window}"] = vol
            features[f"volatility_{window}_ratio"] = vol / vol.rolling(50).mean()

        # ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™
        # RSI
        delta = data["Close"].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / loss
        features["rsi"] = 100 - (100 / (1 + rs))
        features["rsi_sma"] = features["rsi"].rolling(5).mean()

        # MACD
        ema_12 = data["Close"].ewm(span=12).mean()
        ema_26 = data["Close"].ewm(span=26).mean()
        features["macd"] = ema_12 - ema_26
        features["macd_signal"] = features["macd"].ewm(span=9).mean()
        features["macd_histogram"] = features["macd"] - features["macd_signal"]

        # ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰
        sma_20 = features["sma_20"]
        std_20 = data["Close"].rolling(20).std()
        features["bb_upper"] = sma_20 + (std_20 * 2)
        features["bb_lower"] = sma_20 - (std_20 * 2)
        features["bb_position"] = (data["Close"] - features["bb_lower"]) / (
            features["bb_upper"] - features["bb_lower"]
        )
        features["bb_width"] = (features["bb_upper"] - features["bb_lower"]) / sma_20

        # å‡ºæ¥é«˜ç‰¹å¾´é‡
        features["volume_sma_20"] = data["Volume"].rolling(20).mean()
        features["volume_ratio"] = data["Volume"] / features["volume_sma_20"]
        features["volume_price_trend"] = (
            features["return_1d"] * features["volume_ratio"]
        )

        # é«˜å€¤ãƒ»å®‰å€¤ç‰¹å¾´é‡
        for window in [5, 10, 20]:
            features[f"high_{window}"] = data["High"].rolling(window).max()
            features[f"low_{window}"] = data["Low"].rolling(window).min()
            features[f"price_position_{window}"] = (
                data["Close"] - features[f"low_{window}"]
            ) / (features[f"high_{window}"] - features[f"low_{window}"])

        # ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ æŒ‡æ¨™
        for window in [5, 10, 20]:
            features[f"momentum_{window}"] = (
                data["Close"] / data["Close"].shift(window) - 1
            )
            features[f"roc_{window}"] = data["Close"].pct_change(window)

        # ãƒ©ã‚°ç‰¹å¾´é‡ï¼ˆé‡è¦ï¼ï¼‰
        for lag in [1, 2, 3, 5]:
            features[f"return_1d_lag_{lag}"] = features["return_1d"].shift(lag)
            features[f"volume_ratio_lag_{lag}"] = features["volume_ratio"].shift(lag)
            features[f"rsi_lag_{lag}"] = features["rsi"].shift(lag)

        # çµ±è¨ˆçš„ç‰¹å¾´é‡
        for window in [5, 10, 20]:
            returns = data["Close"].pct_change()
            features[f"return_mean_{window}"] = returns.rolling(window).mean()
            features[f"return_std_{window}"] = returns.rolling(window).std()
            features[f"return_skew_{window}"] = returns.rolling(window).skew()
            features[f"return_kurt_{window}"] = returns.rolling(window).kurt()

        # ç›¸å¯¾å¼·åº¦æŒ‡æ¨™
        features["price_rank_20"] = data["Close"].rolling(20).rank(pct=True)
        features["volume_rank_20"] = data["Volume"].rolling(20).rank(pct=True)

        # ãƒˆãƒ¬ãƒ³ãƒ‰ç‰¹å¾´é‡
        for window in [10, 20]:
            poly_coef = []
            for i in range(window, len(data)):
                y = data["Close"].iloc[i - window : i].values
                x = np.arange(len(y))
                coef = np.polyfit(x, y, 1)[0]  # ç·šå½¢ãƒˆãƒ¬ãƒ³ãƒ‰ä¿‚æ•°
                poly_coef.append(coef)

            trend_series = pd.Series([np.nan] * window + poly_coef, index=data.index)
            features[f"trend_coef_{window}"] = trend_series

        return features

    def create_target_variable(
        self, data: pd.DataFrame, prediction_days: int = 7
    ) -> pd.Series:
        """ç›®æ¨™å¤‰æ•°ä½œæˆï¼ˆä¸­æœŸãƒªã‚¿ãƒ¼ãƒ³äºˆæ¸¬ï¼‰"""
        # 7æ—¥å¾Œã®ç´¯ç©ãƒªã‚¿ãƒ¼ãƒ³ç‡
        future_return = data["Close"].shift(-prediction_days) / data["Close"] - 1
        return future_return

    def prepare_training_data(self, symbol: str) -> Tuple[np.ndarray, np.ndarray]:
        """è¨“ç·´ãƒ‡ãƒ¼ã‚¿æº–å‚™"""
        print(f"ãƒ‡ãƒ¼ã‚¿æº–å‚™ä¸­: {symbol}")

        # ã‚ˆã‚Šé•·æœŸã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        data = self.data_provider.get_stock_data(symbol, "2y")
        if data.empty or len(data) < 200:
            return None, None

        # ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™è¿½åŠ 
        data = self.data_provider.calculate_technical_indicators(data)

        # åŒ…æ‹¬çš„ç‰¹å¾´é‡ä½œæˆ
        features = self.create_comprehensive_features(data)

        # ç›®æ¨™å¤‰æ•°ä½œæˆï¼ˆ7æ—¥å¾Œãƒªã‚¿ãƒ¼ãƒ³ï¼‰
        target = self.create_target_variable(data, prediction_days=7)

        # ç‰¹å¾´é‡é¸æŠï¼ˆæ•°å€¤ã®ã¿ã€æœªæ¥æƒ…å ±ãªã—ï¼‰
        numeric_features = features.select_dtypes(include=[np.number])

        # ç›®æ¨™å¤‰æ•°ã¨åŒã˜æœŸé–“ã«èª¿æ•´
        aligned_features = numeric_features.align(target, join="inner", axis=0)[0]

        # NaNé™¤å»
        combined = pd.concat([aligned_features, target.rename("target")], axis=1)
        combined = combined.dropna()

        if len(combined) < 50:
            return None, None

        # X, yåˆ†é›¢
        X = combined.iloc[:, :-1].values
        y = combined.iloc[:, -1].values

        print(f"  ç‰¹å¾´é‡æ•°: {X.shape[1]}")
        print(f"  ã‚µãƒ³ãƒ—ãƒ«æ•°: {X.shape[0]}")

        return X, y

    def train_models(self, symbols: List[str]) -> Dict:
        """è¤‡æ•°ãƒ¢ãƒ‡ãƒ«è¨“ç·´"""
        print("=" * 60)
        print("æ­£ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã®æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«è¨“ç·´")
        print("=" * 60)

        # ãƒ‡ãƒ¼ã‚¿åé›†
        all_X = []
        all_y = []

        for symbol in symbols[:10]:  # 10éŠ˜æŸ„
            X, y = self.prepare_training_data(symbol)
            if X is not None and y is not None:
                all_X.append(X)
                all_y.append(y)

        if not all_X:
            return {"error": "No training data"}

        # ãƒ‡ãƒ¼ã‚¿çµåˆ
        X_combined = np.vstack(all_X)
        y_combined = np.hstack(all_y)

        print(
            f"ç·è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {X_combined.shape[0]}ã‚µãƒ³ãƒ—ãƒ«, {X_combined.shape[1]}ç‰¹å¾´é‡"
        )

        # å¤–ã‚Œå€¤é™¤å»ï¼ˆé‡è¦ï¼ï¼‰
        q1, q3 = np.percentile(y_combined, [25, 75])
        iqr = q3 - q1
        lower_bound = q1 - 1.5 * iqr
        upper_bound = q3 + 1.5 * iqr

        mask = (y_combined >= lower_bound) & (y_combined <= upper_bound)
        X_cleaned = X_combined[mask]
        y_cleaned = y_combined[mask]

        print(f"å¤–ã‚Œå€¤é™¤å»å¾Œ: {X_cleaned.shape[0]}ã‚µãƒ³ãƒ—ãƒ«")

        # ãƒ¢ãƒ‡ãƒ«å®šç¾©
        models = {
            "random_forest": RandomForestRegressor(
                n_estimators=200, max_depth=10, random_state=42, n_jobs=-1
            ),
            "gradient_boosting": GradientBoostingRegressor(
                n_estimators=200, learning_rate=0.1, max_depth=6, random_state=42
            ),
            "ridge": Ridge(alpha=1.0),
            "linear": LinearRegression(),
        }

        # æ™‚ç³»åˆ—åˆ†å‰²æ¤œè¨¼
        tscv = TimeSeriesSplit(n_splits=3)
        best_model = None
        best_mape = float("inf")
        best_name = ""

        for name, model in models.items():
            print(f"\n{name}ãƒ¢ãƒ‡ãƒ«è¨“ç·´ä¸­...")

            mape_scores = []

            for train_idx, val_idx in tscv.split(X_cleaned):
                X_train, X_val = X_cleaned[train_idx], X_cleaned[val_idx]
                y_train, y_val = y_cleaned[train_idx], y_cleaned[val_idx]

                # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
                scaler = StandardScaler()
                X_train_scaled = scaler.fit_transform(X_train)
                X_val_scaled = scaler.transform(X_val)

                # è¨“ç·´
                model.fit(X_train_scaled, y_train)

                # äºˆæ¸¬
                y_pred = model.predict(X_val_scaled)

                # MAPEè¨ˆç®—ï¼ˆæ­£ã—ã„æ–¹æ³•ï¼‰
                # ã‚¼ãƒ­é™¤ç®—å›é¿ï¼šå°ã•ã™ãã‚‹å€¤ã‚’é™¤å¤–
                mask = np.abs(y_val) > 0.01  # 1%ä»¥ä¸Šã®å¤‰å‹•ã®ã¿
                if mask.sum() > 10:  # ååˆ†ãªã‚µãƒ³ãƒ—ãƒ«
                    mape = (
                        mean_absolute_percentage_error(y_val[mask], y_pred[mask]) * 100
                    )
                    mape_scores.append(mape)

            if mape_scores:
                avg_mape = np.mean(mape_scores)
                print(f"  å¹³å‡MAPE: {avg_mape:.2f}%")

                if avg_mape < best_mape:
                    best_mape = avg_mape
                    best_model = model
                    best_name = name

        # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã§å…¨ãƒ‡ãƒ¼ã‚¿è¨“ç·´
        if best_model is not None:
            print(f"\næœ€è‰¯ãƒ¢ãƒ‡ãƒ«: {best_name} (MAPE: {best_mape:.2f}%)")

            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X_cleaned)
            best_model.fit(X_scaled, y_cleaned)

            self.models["main"] = best_model
            self.scalers["main"] = scaler
            self.is_trained = True

            return {
                "best_model": best_name,
                "mape": best_mape,
                "training_samples": len(X_cleaned),
            }

        return {"error": "Training failed"}

    def predict_return(self, symbol: str) -> float:
        """ãƒªã‚¿ãƒ¼ãƒ³ç‡äºˆæ¸¬"""
        if not self.is_trained:
            return 0.0

        try:
            # ãƒ‡ãƒ¼ã‚¿å–å¾—
            data = self.data_provider.get_stock_data(symbol, "6mo")
            if data.empty or len(data) < 100:
                return 0.0

            data = self.data_provider.calculate_technical_indicators(data)

            # ç‰¹å¾´é‡ä½œæˆ
            features = self.create_comprehensive_features(data)

            # æœ€æ–°ãƒ‡ãƒ¼ã‚¿
            latest_features = (
                features.select_dtypes(include=[np.number]).iloc[-1:].fillna(0)
            )

            # äºˆæ¸¬
            features_scaled = self.scalers["main"].transform(latest_features)
            prediction = self.models["main"].predict(features_scaled)[0]

            # ç¾å®Ÿçš„ç¯„å›²ã«åˆ¶é™
            return max(-0.2, min(0.2, prediction))

        except Exception as e:
            logger.error(f"Error predicting for {symbol}: {str(e)}")
            return 0.0

    def test_final_system(self, symbols: List[str]) -> Dict:
        """æœ€çµ‚ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ"""
        print("\næœ€çµ‚ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ")
        print("-" * 40)

        predictions = []
        actuals = []
        valid_mapes = []

        for symbol in symbols[:5]:
            try:
                data = self.data_provider.get_stock_data(symbol, "1y")
                if len(data) < 100:
                    continue

                # è¤‡æ•°ã®ãƒ†ã‚¹ãƒˆãƒã‚¤ãƒ³ãƒˆ
                for i in range(50, 10, -7):  # 7æ—¥ãšã¤
                    historical_data = data.iloc[:-i].copy()

                    if len(historical_data) < 50:
                        continue

                    # å®Ÿéš›ã®7æ—¥ãƒªã‚¿ãƒ¼ãƒ³
                    start_price = data.iloc[-i]["Close"]
                    end_price = (
                        data.iloc[-i + 7]["Close"] if i >= 7 else data.iloc[-1]["Close"]
                    )
                    actual_return = (end_price - start_price) / start_price

                    # äºˆæ¸¬ï¼ˆã“ã®å®Ÿè£…ã§ã¯ç°¡ç•¥åŒ–ï¼‰
                    predicted_return = self._simple_ml_predict(historical_data)

                    predictions.append(predicted_return)
                    actuals.append(actual_return)

                    # æœ‰åŠ¹MAPE
                    if abs(actual_return) > 0.01:  # 1%ä»¥ä¸Š
                        mape_individual = (
                            abs((actual_return - predicted_return) / actual_return)
                            * 100
                        )
                        valid_mapes.append(mape_individual)

            except Exception as e:
                logger.warning(f"Error testing {symbol}: {str(e)}")
                continue

        if valid_mapes:
            final_mape = np.mean(valid_mapes)
            mae = np.mean(np.abs(np.array(predictions) - np.array(actuals)))

            return {
                "mape": final_mape,
                "mae": mae,
                "total_tests": len(predictions),
                "valid_tests": len(valid_mapes),
            }

        return {"error": "No valid tests"}

    def _simple_ml_predict(self, data: pd.DataFrame) -> float:
        """ç°¡æ˜“MLäºˆæ¸¬ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰"""
        try:
            data = self.data_provider.calculate_technical_indicators(data)
            returns = data["Close"].pct_change().dropna()

            if len(returns) < 20:
                return 0.0

            # ç°¡å˜ãªãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜
            recent_trend = returns.iloc[-7:].mean()
            volatility = returns.iloc[-20:].std()
            momentum = (data["Close"].iloc[-1] - data["Close"].iloc[-7]) / data[
                "Close"
            ].iloc[-7]

            # ç·šå½¢çµåˆ
            prediction = recent_trend * 0.3 + momentum * 0.5

            # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£èª¿æ•´
            if volatility > 0.03:
                prediction *= 0.7

            return max(-0.1, min(0.1, prediction))

        except Exception:
            return 0.0


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("=" * 60)
    print("æ­£ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§MAPE 10-20%é”æˆãƒãƒ£ãƒ¬ãƒ³ã‚¸")
    print("=" * 60)

    data_provider = StockDataProvider()
    symbols = list(data_provider.jp_stock_codes.keys())

    predictor = CorrectMAPEPredictor()

    # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
    train_results = predictor.train_models(symbols)

    if "error" not in train_results:
        print(f"\nè¨“ç·´çµæœ:")
        print(f"  æœ€è‰¯ãƒ¢ãƒ‡ãƒ«: {train_results['best_model']}")
        print(f"  MAPE: {train_results['mape']:.2f}%")
        print(f"  è¨“ç·´ã‚µãƒ³ãƒ—ãƒ«: {train_results['training_samples']}")

        # ãƒ†ã‚¹ãƒˆ
        test_results = predictor.test_final_system(symbols)

        if "error" not in test_results:
            print(f"\nãƒ†ã‚¹ãƒˆçµæœ:")
            print(f"  MAPE: {test_results['mape']:.2f}%")
            print(f"  MAE: {test_results['mae']:.4f}")
            print(f"  ç·ãƒ†ã‚¹ãƒˆ: {test_results['total_tests']}")
            print(f"  æœ‰åŠ¹ãƒ†ã‚¹ãƒˆ: {test_results['valid_tests']}")

            if test_results["mape"] < 20:
                if test_results["mape"] < 10:
                    print("ğŸ‰ ç›®æ¨™é”æˆï¼MAPE < 10%")
                else:
                    print("âœ“ è‰¯å¥½ãªçµæœï¼MAPE < 20%")
            else:
                print("ç¶™ç¶šæ”¹å–„ãŒå¿…è¦")

        # ç¾åœ¨ã®äºˆæ¸¬ä¾‹
        print(f"\nç¾åœ¨ã®äºˆæ¸¬ä¾‹:")
        print("-" * 30)

        for symbol in symbols[:5]:
            pred_return = predictor.predict_return(symbol)
            print(
                f"{symbol}: 7æ—¥å¾Œãƒªã‚¿ãƒ¼ãƒ³äºˆæ¸¬ {pred_return:.3f} ({pred_return*100:.1f}%)"
            )

    else:
        print(f"è¨“ç·´å¤±æ•—: {train_results}")


if __name__ == "__main__":
    main()
