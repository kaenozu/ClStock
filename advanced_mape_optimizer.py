#!/usr/bin/env python3
"""
é«˜åº¦ãªãƒ‡ãƒ¼ã‚¿å“è³ªã¨ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã§MAPE 10-20%é”æˆ
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor
from sklearn.linear_model import ElasticNet
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.feature_selection import SelectKBest, f_regression
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_absolute_percentage_error
import logging
from typing import Dict, List, Tuple
import warnings
warnings.filterwarnings('ignore')

from data.stock_data import StockDataProvider

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class AdvancedMAPEOptimizer:
    """é«˜åº¦ãªMAPEæœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        self.data_provider = StockDataProvider()
        self.model = None
        self.scaler = None
        self.feature_selector = None
        self.is_trained = False

    def create_advanced_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """é«˜åº¦ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°"""
        features = pd.DataFrame(index=data.index)

        # åŸºæœ¬ä¾¡æ ¼ç‰¹å¾´é‡
        close = data['Close']
        volume = data['Volume']
        high = data['High']
        low = data['Low']

        # 1. ä¾¡æ ¼å¤‰åŒ–ç‡ï¼ˆè¤‡æ•°æœŸé–“ï¼‰
        for period in [1, 2, 3, 5, 7, 10, 14, 21]:
            features[f'return_{period}d'] = close.pct_change(period)
            features[f'log_return_{period}d'] = np.log(close / close.shift(period))

        # 2. ç§»å‹•å¹³å‡ç³»ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        for window in [5, 10, 20, 50]:
            sma = close.rolling(window).mean()
            ema = close.ewm(span=window).mean()

            features[f'sma_{window}'] = sma
            features[f'ema_{window}'] = ema
            features[f'price_sma_{window}_ratio'] = close / sma
            features[f'price_ema_{window}_ratio'] = close / ema

            # ç§»å‹•å¹³å‡ã®å‚¾ã
            features[f'sma_{window}_slope'] = (sma - sma.shift(5)) / sma.shift(5)
            features[f'ema_{window}_slope'] = (ema - ema.shift(5)) / ema.shift(5)

            # ç§»å‹•å¹³å‡ã‹ã‚‰ã®è·é›¢ï¼ˆæ¨™æº–åŒ–ï¼‰
            std = close.rolling(window).std()
            features[f'price_sma_{window}_zscore'] = (close - sma) / std

        # 3. ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ç³»ï¼ˆé«˜åº¦ï¼‰
        returns = close.pct_change()
        for window in [5, 10, 20]:
            vol = returns.rolling(window).std()
            features[f'volatility_{window}'] = vol

            # å®Ÿç¾ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
            features[f'realized_vol_{window}'] = np.sqrt(252) * vol

            # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã®å¤‰åŒ–ç‡
            features[f'vol_{window}_change'] = vol.pct_change(5)

            # é«˜å€¤å®‰å€¤ãƒ¬ãƒ³ã‚¸ãƒ™ãƒ¼ã‚¹ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
            hl_vol = np.log(high / low).rolling(window).mean()
            features[f'hl_volatility_{window}'] = hl_vol

        # 4. é«˜åº¦ãªãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™
        # RSIï¼ˆè¤‡æ•°æœŸé–“ï¼‰
        for period in [7, 14, 21]:
            delta = close.diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
            rs = gain / loss
            rsi = 100 - (100 / (1 + rs))
            features[f'rsi_{period}'] = rsi

            # RSIã®å¤‰åŒ–ç‡
            features[f'rsi_{period}_change'] = rsi.diff(5)

        # MACDç³»ï¼ˆè¤‡æ•°è¨­å®šï¼‰
        for fast, slow, signal in [(12, 26, 9), (8, 21, 5)]:
            ema_fast = close.ewm(span=fast).mean()
            ema_slow = close.ewm(span=slow).mean()
            macd = ema_fast - ema_slow
            macd_signal = macd.ewm(span=signal).mean()

            features[f'macd_{fast}_{slow}'] = macd
            features[f'macd_signal_{fast}_{slow}'] = macd_signal
            features[f'macd_hist_{fast}_{slow}'] = macd - macd_signal

        # 5. å‡ºæ¥é«˜åˆ†æï¼ˆé«˜åº¦ï¼‰
        vol_sma_20 = volume.rolling(20).mean()
        features['volume_ratio'] = volume / vol_sma_20

        # å‡ºæ¥é«˜åŠ é‡å¹³å‡ä¾¡æ ¼
        vwap = (close * volume).rolling(20).sum() / volume.rolling(20).sum()
        features['vwap'] = vwap
        features['price_vwap_ratio'] = close / vwap

        # å‡ºæ¥é«˜ãƒˆãƒ¬ãƒ³ãƒ‰
        features['volume_trend'] = volume.rolling(20).apply(lambda x: np.polyfit(range(len(x)), x, 1)[0])

        # 6. ã‚µãƒãƒ¼ãƒˆãƒ»ãƒ¬ã‚¸ã‚¹ã‚¿ãƒ³ã‚¹
        for window in [10, 20, 50]:
            rolling_max = high.rolling(window).max()
            rolling_min = low.rolling(window).min()

            features[f'support_resistance_{window}'] = (close - rolling_min) / (rolling_max - rolling_min)
            features[f'distance_to_high_{window}'] = (rolling_max - close) / close
            features[f'distance_to_low_{window}'] = (close - rolling_min) / close

        # 7. å­£ç¯€æ€§ãƒ»å‘¨æœŸæ€§
        features['day_of_week'] = pd.to_datetime(data.index).dayofweek
        features['month'] = pd.to_datetime(data.index).month
        features['quarter'] = pd.to_datetime(data.index).quarter

        # æœˆæœ«åŠ¹æœ
        month_end = pd.to_datetime(data.index).is_month_end.astype(int)
        features['month_end'] = month_end

        # 8. ãƒ©ã‚°ç‰¹å¾´é‡ï¼ˆé‡è¦ãªéå»æƒ…å ±ï¼‰
        important_features = ['return_1d', 'return_5d', 'volume_ratio', 'rsi_14']
        for feature in important_features:
            if feature in features.columns:
                for lag in [1, 2, 3, 5, 7]:
                    features[f'{feature}_lag_{lag}'] = features[feature].shift(lag)

        # 9. ç›¸äº’ä½œç”¨ç‰¹å¾´é‡
        features['price_volume_interaction'] = features['return_1d'] * features['volume_ratio']
        features['rsi_volume_interaction'] = features['rsi_14'] * features['volume_ratio']

        # 10. çµ±è¨ˆçš„ç‰¹å¾´é‡ï¼ˆä¿®æ­£ç‰ˆï¼‰
        for window in [10, 20]:
            # æ­ªåº¦ï¼ˆskewï¼‰ã®ã¿ä½¿ç”¨ï¼ˆkurtosisã¯é™¤å¤–ï¼‰
            features[f'return_skew_{window}'] = returns.rolling(window).skew()

            # ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«
            features[f'price_quantile_{window}'] = close.rolling(window).rank(pct=True)

            # ä»£æ›¿çµ±è¨ˆé‡ã¨ã—ã¦æ¨™æº–åŒ–ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆ
            rolling_returns = returns.rolling(window)
            mean_return = rolling_returns.mean()
            std_return = rolling_returns.std()
            features[f'return_normalized_{window}'] = (returns - mean_return) / std_return

        return features

    def clean_and_select_features(self, features: pd.DataFrame, target: pd.Series) -> Tuple[pd.DataFrame, List[str]]:
        """ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã¨ç‰¹å¾´é‡é¸æŠ"""

        # æ•°å€¤ç‰¹å¾´é‡ã®ã¿é¸æŠ
        numeric_features = features.select_dtypes(include=[np.number])

        # ç„¡é™å¤§ã¨NaNã®å‡¦ç†
        numeric_features = numeric_features.replace([np.inf, -np.inf], np.nan)

        # ç›®æ¨™å¤‰æ•°ã¨æœŸé–“ã‚’åˆã‚ã›ã‚‹
        aligned_features, aligned_target = numeric_features.align(target, join='inner', axis=0)

        # NaNé™¤å»
        combined = pd.concat([aligned_features, aligned_target.rename('target')], axis=1)
        cleaned = combined.dropna()

        if len(cleaned) < 100:
            return None, None

        X = cleaned.iloc[:, :-1]
        y = cleaned.iloc[:, -1]

        # ä½åˆ†æ•£ç‰¹å¾´é‡é™¤å»
        variance_threshold = 1e-8
        feature_variances = X.var()
        high_variance_features = feature_variances[feature_variances > variance_threshold].index.tolist()
        X_filtered = X[high_variance_features]

        # ç‰¹å¾´é‡é¸æŠï¼ˆä¸Šä½Kå€‹ï¼‰
        if len(X_filtered.columns) > 50:
            selector = SelectKBest(score_func=f_regression, k=50)
            X_selected = selector.fit_transform(X_filtered, y)
            selected_features = X_filtered.columns[selector.get_support()].tolist()
        else:
            X_selected = X_filtered
            selected_features = X_filtered.columns.tolist()

        return pd.DataFrame(X_selected, index=X.index, columns=selected_features), selected_features

    def optimize_prediction_target(self, data: pd.DataFrame) -> pd.Series:
        """äºˆæ¸¬å¯¾è±¡ã®æœ€é©åŒ–"""
        close = data['Close']

        # è¤‡æ•°ã®å¯¾è±¡ã‚’è©¦ã—ã¦æœ€ã‚‚äºˆæ¸¬ã—ã‚„ã™ã„ã‚‚ã®ã‚’é¸æŠ
        targets = {}

        # 1. æ¨™æº–çš„ãªãƒªã‚¿ãƒ¼ãƒ³
        targets['return_7d'] = close.shift(-7) / close - 1

        # 2. ãƒ­ã‚°ãƒªã‚¿ãƒ¼ãƒ³
        targets['log_return_7d'] = np.log(close.shift(-7) / close)

        # 3. ä¾¡æ ¼å¤‰åŒ–ã®æ–¹å‘ï¼ˆåˆ†é¡ã¨ã—ã¦ï¼‰
        targets['direction_7d'] = np.sign(close.shift(-7) - close)

        # 4. åŒºé–“ãƒªã‚¿ãƒ¼ãƒ³ï¼ˆã‚ˆã‚Šå®‰å®šï¼‰
        targets['cumulative_return_7d'] = (close.shift(-7) / close - 1)

        # æš«å®šçš„ã«æ¨™æº–ãƒªã‚¿ãƒ¼ãƒ³ã‚’ä½¿ç”¨
        return targets['return_7d']

    def train_optimized_model(self, symbols: List[str]) -> Dict:
        """æœ€é©åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«è¨“ç·´"""
        print("é«˜åº¦ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«è¨“ç·´")
        print("-" * 50)

        all_features = []
        all_targets = []
        feature_names = None

        # ãƒ‡ãƒ¼ã‚¿åé›†ã¨å‰å‡¦ç†
        for symbol in symbols[:5]:  # è¨ˆç®—é‡è€ƒæ…®ã§5éŠ˜æŸ„
            try:
                print(f"å‡¦ç†ä¸­: {symbol}")

                # ã‚ˆã‚Šé•·æœŸã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                data = self.data_provider.get_stock_data(symbol, "3y")
                if len(data) < 500:
                    continue

                data = self.data_provider.calculate_technical_indicators(data)

                # é«˜åº¦ãªç‰¹å¾´é‡ä½œæˆ
                features = self.create_advanced_features(data)

                # æœ€é©åŒ–ã•ã‚ŒãŸç›®æ¨™å¤‰æ•°
                target = self.optimize_prediction_target(data)

                # ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã¨ç‰¹å¾´é‡é¸æŠ
                cleaned_features, selected_features = self.clean_and_select_features(features, target)

                if cleaned_features is not None:
                    all_features.append(cleaned_features.values)
                    all_targets.append(target.loc[cleaned_features.index].values)

                    if feature_names is None:
                        feature_names = selected_features

                    print(f"  ç‰¹å¾´é‡æ•°: {len(selected_features)}, ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(cleaned_features)}")

            except Exception as e:
                logger.warning(f"Error processing {symbol}: {str(e)}")
                continue

        if not all_features:
            return {"error": "No valid data"}

        # ãƒ‡ãƒ¼ã‚¿çµ±åˆ
        X_combined = np.vstack(all_features)
        y_combined = np.hstack(all_targets)

        print(f"\nçµ±åˆãƒ‡ãƒ¼ã‚¿: {X_combined.shape[0]}ã‚µãƒ³ãƒ—ãƒ«, {X_combined.shape[1]}ç‰¹å¾´é‡")

        # å¤–ã‚Œå€¤é™¤å»ï¼ˆã‚ˆã‚Šå³æ ¼ï¼‰
        q1, q3 = np.percentile(y_combined, [10, 90])  # ã‚ˆã‚Šå³æ ¼ãª10-90ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«
        iqr = q3 - q1
        lower_bound = q1 - 1.5 * iqr
        upper_bound = q3 + 1.5 * iqr

        mask = (y_combined >= lower_bound) & (y_combined <= upper_bound)
        X_clean = X_combined[mask]
        y_clean = y_combined[mask]

        print(f"å¤–ã‚Œå€¤é™¤å»å¾Œ: {X_clean.shape[0]}ã‚µãƒ³ãƒ—ãƒ«")

        # é«˜åº¦ãªãƒ¢ãƒ‡ãƒ«
        models = {
            'extra_trees': ExtraTreesRegressor(
                n_estimators=300,
                max_depth=12,
                min_samples_split=5,
                min_samples_leaf=2,
                random_state=42,
                n_jobs=-1
            ),
            'random_forest': RandomForestRegressor(
                n_estimators=300,
                max_depth=15,
                min_samples_split=5,
                min_samples_leaf=2,
                random_state=42,
                n_jobs=-1
            ),
            'elastic_net': ElasticNet(
                alpha=0.01,
                l1_ratio=0.5,
                random_state=42
            )
        }

        # æ™‚ç³»åˆ—åˆ†å‰²ã§ã®è©•ä¾¡
        tscv = TimeSeriesSplit(n_splits=4)
        best_model = None
        best_mape = float('inf')
        best_name = ""

        for name, model in models.items():
            print(f"\n{name}è©•ä¾¡ä¸­...")

            mape_scores = []

            for train_idx, val_idx in tscv.split(X_clean):
                X_train, X_val = X_clean[train_idx], X_clean[val_idx]
                y_train, y_val = y_clean[train_idx], y_clean[val_idx]

                # å …ç‰¢ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
                scaler = RobustScaler()
                X_train_scaled = scaler.fit_transform(X_train)
                X_val_scaled = scaler.transform(X_val)

                # è¨“ç·´
                model.fit(X_train_scaled, y_train)

                # äºˆæ¸¬
                y_pred = model.predict(X_val_scaled)

                # MAPEè¨ˆç®—ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
                # ã‚ˆã‚Šå¤§ããªé–¾å€¤ã§å®‰å®šæ€§å‘ä¸Š
                mask = np.abs(y_val) > 0.02  # 2%ä»¥ä¸Šã®å¤‰å‹•
                if mask.sum() > 20:  # ååˆ†ãªã‚µãƒ³ãƒ—ãƒ«
                    try:
                        mape = mean_absolute_percentage_error(y_val[mask], y_pred[mask]) * 100
                        mape_scores.append(mape)
                    except:
                        continue

            if mape_scores:
                avg_mape = np.mean(mape_scores)
                std_mape = np.std(mape_scores)
                print(f"  MAPE: {avg_mape:.2f}% Â± {std_mape:.2f}%")

                if avg_mape < best_mape:
                    best_mape = avg_mape
                    best_model = model
                    best_name = name

        # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã§å…¨ãƒ‡ãƒ¼ã‚¿å†è¨“ç·´
        if best_model is not None:
            print(f"\næœ€è‰¯ãƒ¢ãƒ‡ãƒ«: {best_name} (MAPE: {best_mape:.2f}%)")

            scaler = RobustScaler()
            X_final = scaler.fit_transform(X_clean)
            best_model.fit(X_final, y_clean)

            self.model = best_model
            self.scaler = scaler
            self.feature_names = feature_names
            self.is_trained = True

            return {
                'best_model': best_name,
                'mape': best_mape,
                'training_samples': len(X_clean),
                'features_count': len(feature_names)
            }

        return {"error": "Training failed"}

    def predict_advanced(self, symbol: str) -> float:
        """é«˜åº¦ãªäºˆæ¸¬"""
        if not self.is_trained:
            return 0.0

        try:
            data = self.data_provider.get_stock_data(symbol, "1y")
            if len(data) < 100:
                return 0.0

            data = self.data_provider.calculate_technical_indicators(data)

            # åŒã˜ç‰¹å¾´é‡ã‚’ä½œæˆ
            features = self.create_advanced_features(data)

            # å¿…è¦ãªç‰¹å¾´é‡ã®ã¿æŠ½å‡º
            if hasattr(self, 'feature_names'):
                feature_subset = features[self.feature_names].iloc[-1:].fillna(0)
            else:
                feature_subset = features.select_dtypes(include=[np.number]).iloc[-1:].fillna(0)

            # äºˆæ¸¬
            features_scaled = self.scaler.transform(feature_subset)
            prediction = self.model.predict(features_scaled)[0]

            return max(-0.15, min(0.15, prediction))

        except Exception as e:
            logger.error(f"Error in advanced prediction for {symbol}: {str(e)}")
            return 0.0

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("=" * 60)
    print("é«˜åº¦ãªãƒ‡ãƒ¼ã‚¿å“è³ªã¨ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã§MAPEæœ€é©åŒ–")
    print("=" * 60)

    data_provider = StockDataProvider()
    symbols = list(data_provider.jp_stock_codes.keys())

    optimizer = AdvancedMAPEOptimizer()

    # æœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«è¨“ç·´
    results = optimizer.train_optimized_model(symbols)

    if 'error' not in results:
        print(f"\næœ€çµ‚çµæœ:")
        print(f"  æœ€è‰¯ãƒ¢ãƒ‡ãƒ«: {results['best_model']}")
        print(f"  MAPE: {results['mape']:.2f}%")
        print(f"  è¨“ç·´ã‚µãƒ³ãƒ—ãƒ«: {results['training_samples']}")
        print(f"  ç‰¹å¾´é‡æ•°: {results['features_count']}")

        if results['mape'] < 20:
            if results['mape'] < 15:
                if results['mape'] < 10:
                    print("ğŸ‰ ç´ æ™´ã‚‰ã—ã„ï¼MAPE < 10%é”æˆï¼")
                else:
                    print("ğŸ‰ å„ªç§€ï¼MAPE < 15%é”æˆï¼")
            else:
                print("âœ“ è‰¯å¥½ï¼MAPE < 20%é”æˆï¼")
        else:
            print("æ›´ãªã‚‹æœ€é©åŒ–ãŒå¿…è¦")

        # äºˆæ¸¬ä¾‹
        print(f"\näºˆæ¸¬ä¾‹:")
        print("-" * 20)
        for symbol in symbols[:5]:
            pred = optimizer.predict_advanced(symbol)
            print(f"{symbol}: {pred:.3f} ({pred*100:.1f}%)")

    else:
        print(f"ã‚¨ãƒ©ãƒ¼: {results}")

if __name__ == "__main__":
    main()