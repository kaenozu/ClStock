#!/usr/bin/env python3
"""
é«˜ç²¾åº¦æ–¹å‘æ€§äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ï¼ˆ80%ä»¥ä¸Šã®ç²¾åº¦ã‚’ç›®æŒ‡ã™ï¼‰
ä¸Šæ˜‡ãƒ»ä¸‹é™ã®äºˆæ¸¬ã«ç‰¹åŒ–ã—ãŸæœ€é©åŒ–ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple
import logging
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import accuracy_score, classification_report
import warnings

warnings.filterwarnings("ignore")

from data.stock_data import StockDataProvider

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class AdvancedDirectionalPredictor:
    """é«˜ç²¾åº¦æ–¹å‘æ€§äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        self.data_provider = StockDataProvider()
        self.scaler = StandardScaler()

    def create_directional_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """æ–¹å‘æ€§äºˆæ¸¬ã«æœ€é©åŒ–ã•ã‚ŒãŸç‰¹å¾´é‡"""
        features = pd.DataFrame(index=data.index)

        close = data["Close"]
        volume = data["Volume"]
        high = data["High"]
        low = data["Low"]

        # 1. è¤‡æ•°æœŸé–“ã®ç§»å‹•å¹³å‡é–¢ä¿‚
        sma_5 = close.rolling(5).mean()
        sma_10 = close.rolling(10).mean()
        sma_20 = close.rolling(20).mean()
        sma_50 = close.rolling(50).mean()

        # ç§»å‹•å¹³å‡ã®åºåˆ—ï¼ˆå¼·åŠ›ãªãƒˆãƒ¬ãƒ³ãƒ‰æŒ‡æ¨™ï¼‰
        features["ma_alignment_bull"] = (
            (sma_5 > sma_10) & (sma_10 > sma_20) & (sma_20 > sma_50)
        ).astype(int)
        features["ma_alignment_bear"] = (
            (sma_5 < sma_10) & (sma_10 < sma_20) & (sma_20 < sma_50)
        ).astype(int)

        # ç§»å‹•å¹³å‡ã‹ã‚‰ã®è·é›¢
        features["price_above_sma20"] = (close > sma_20).astype(int)
        features["price_above_sma50"] = (close > sma_50).astype(int)

        # 2. ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ æŒ‡æ¨™
        returns_1d = close.pct_change()
        returns_3d = close.pct_change(3)
        returns_5d = close.pct_change(5)

        # é€£ç¶šä¸Šæ˜‡/ä¸‹é™ã®æ¤œå‡º
        features["consecutive_up"] = (returns_1d > 0).rolling(3).sum()
        features["consecutive_down"] = (returns_1d < 0).rolling(3).sum()

        # å¼·ã„ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ 
        vol_20 = returns_1d.rolling(20).std()
        features["strong_momentum_up"] = (returns_3d > vol_20 * 0.5).astype(int)
        features["strong_momentum_down"] = (returns_3d < -vol_20 * 0.5).astype(int)

        # 3. ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™
        # RSI
        rsi_14 = self._calculate_rsi(close, 14)
        features["rsi_oversold"] = (rsi_14 < 30).astype(int)
        features["rsi_overbought"] = (rsi_14 > 70).astype(int)
        features["rsi_neutral"] = ((rsi_14 >= 40) & (rsi_14 <= 60)).astype(int)

        # MACD
        ema_12 = close.ewm(span=12).mean()
        ema_26 = close.ewm(span=26).mean()
        macd = ema_12 - ema_26
        macd_signal = macd.ewm(span=9).mean()
        features["macd_bullish"] = (macd > macd_signal).astype(int)
        features["macd_bullish_cross"] = (
            (macd > macd_signal) & (macd.shift(1) <= macd_signal.shift(1))
        ).astype(int)

        # 4. å‡ºæ¥é«˜åˆ†æ
        vol_sma_20 = volume.rolling(20).mean()
        features["high_volume"] = (volume > vol_sma_20 * 1.5).astype(int)
        features["volume_confirm_up"] = (
            (returns_1d > 0) & (volume > vol_sma_20 * 1.2)
        ).astype(int)
        features["volume_confirm_down"] = (
            (returns_1d < 0) & (volume > vol_sma_20 * 1.2)
        ).astype(int)

        # 5. ã‚µãƒãƒ¼ãƒˆãƒ»ãƒ¬ã‚¸ã‚¹ã‚¿ãƒ³ã‚¹
        max_20 = high.rolling(20).max()
        min_20 = low.rolling(20).min()
        features["near_resistance"] = (
            (close > max_20 * 0.98) & (close < max_20 * 1.02)
        ).astype(int)
        features["near_support"] = (
            (close < min_20 * 1.02) & (close > min_20 * 0.98)
        ).astype(int)

        # ä¾¡æ ¼ä½ç½®
        price_position = (close - min_20) / (max_20 - min_20)
        features["price_upper_half"] = (price_position > 0.6).astype(int)
        features["price_lower_half"] = (price_position < 0.4).astype(int)

        # 6. ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£çŠ¶æ³
        current_vol = vol_20
        vol_60 = returns_1d.rolling(60).std()
        features["low_volatility"] = (current_vol < vol_60 * 0.8).astype(int)
        features["high_volatility"] = (current_vol > vol_60 * 1.2).astype(int)

        # 7. ã‚®ãƒ£ãƒƒãƒ—æ¤œå‡º
        prev_close = close.shift(1)
        features["gap_up"] = ((close > prev_close * 1.02)).astype(int)
        features["gap_down"] = ((close < prev_close * 0.98)).astype(int)

        return features

    def _calculate_rsi(self, prices: pd.Series, window: int = 14) -> pd.Series:
        """RSIè¨ˆç®—"""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
        rs = gain / loss
        return 100 - (100 / (1 + rs))

    def create_directional_target(
        self, data: pd.DataFrame, prediction_days: int = 5, threshold: float = 0.01
    ) -> pd.Series:
        """æ–¹å‘æ€§ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼ˆä¸Šæ˜‡/ä¸‹é™/ä¸­ç«‹ï¼‰"""
        close = data["Close"]

        # å°†æ¥ãƒªã‚¿ãƒ¼ãƒ³
        future_return = close.shift(-prediction_days).pct_change(prediction_days)

        # 3ã‚¯ãƒ©ã‚¹åˆ†é¡ï¼ˆé–¾å€¤ãƒ™ãƒ¼ã‚¹ï¼‰
        target = pd.Series(1, index=data.index)  # ä¸­ç«‹=1
        target[future_return > threshold] = 2  # ä¸Šæ˜‡=2
        target[future_return < -threshold] = 0  # ä¸‹é™=0

        return target

    def train_ensemble_classifier(
        self, X: pd.DataFrame, y: pd.Series
    ) -> Tuple[Dict, Dict]:
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«åˆ†é¡å™¨ã®è¨“ç·´"""

        models = {
            "random_forest": RandomForestClassifier(
                n_estimators=200,
                max_depth=10,
                min_samples_split=20,
                min_samples_leaf=10,
                random_state=42,
                class_weight="balanced",
            ),
            "gradient_boosting": GradientBoostingClassifier(
                n_estimators=150, max_depth=6, learning_rate=0.1, random_state=42
            ),
            "logistic": LogisticRegression(
                random_state=42, max_iter=300, class_weight="balanced"
            ),
        }

        # æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        tscv = TimeSeriesSplit(n_splits=3)
        model_scores = {}
        trained_models = {}

        for name, model in models.items():
            accuracies = []
            detailed_scores = []

            for train_idx, val_idx in tscv.split(X):
                X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
                y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

                # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
                X_train_scaled = self.scaler.fit_transform(X_train)
                X_val_scaled = self.scaler.transform(X_val)

                # è¨“ç·´
                model.fit(X_train_scaled, y_train)

                # äºˆæ¸¬
                y_pred = model.predict(X_val_scaled)

                # ç²¾åº¦
                accuracy = accuracy_score(y_val, y_pred)
                accuracies.append(accuracy)

                # ã‚¯ãƒ©ã‚¹åˆ¥ç²¾åº¦
                unique_classes = y_val.unique()
                class_accuracies = {}
                for cls in unique_classes:
                    mask = y_val == cls
                    if mask.sum() > 0:
                        class_acc = (y_pred[mask] == y_val[mask]).mean()
                        class_accuracies[cls] = class_acc

                detailed_scores.append(class_accuracies)

            avg_accuracy = np.mean(accuracies)
            model_scores[name] = {
                "accuracy": avg_accuracy,
                "std": np.std(accuracies),
                "detailed": detailed_scores,
            }

            print(f"  {name}: ç²¾åº¦ {avg_accuracy:.3f} Â± {np.std(accuracies):.3f}")

            # å…¨ãƒ‡ãƒ¼ã‚¿ã§æœ€çµ‚è¨“ç·´
            X_scaled = self.scaler.fit_transform(X)
            model.fit(X_scaled, y)
            trained_models[name] = model

        return model_scores, trained_models

    def test_directional_system(self, symbols: List[str]) -> Dict:
        """æ–¹å‘æ€§äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆ"""
        print("é«˜ç²¾åº¦æ–¹å‘æ€§äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ")
        print("=" * 60)

        all_results = []

        for symbol in symbols[:15]:
            try:
                print(f"\nå‡¦ç†ä¸­: {symbol}")

                # ãƒ‡ãƒ¼ã‚¿å–å¾—
                data = self.data_provider.get_stock_data(symbol, "2y")
                if len(data) < 200:
                    continue

                # ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
                features = self.create_directional_features(data)
                target = self.create_directional_target(
                    data, prediction_days=5, threshold=0.015
                )

                # ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
                valid_idx = ~(features.isna().any(axis=1) | target.isna())
                X = features[valid_idx].fillna(0)
                y = target[valid_idx]

                if len(X) < 100:
                    print(f"  ã‚¹ã‚­ãƒƒãƒ—: ã‚µãƒ³ãƒ—ãƒ«ä¸è¶³ ({len(X)})")
                    continue

                print(f"  æœ‰åŠ¹ã‚µãƒ³ãƒ—ãƒ«: {len(X)}")

                # ã‚¯ãƒ©ã‚¹åˆ†å¸ƒç¢ºèª
                class_counts = y.value_counts()
                print(f"  ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ: {dict(class_counts)}")

                # åˆ†å‰²
                split_idx = int(len(X) * 0.7)
                X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]
                y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]

                if len(X_test) < 20:
                    continue

                # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
                model_scores, trained_models = self.train_ensemble_classifier(
                    X_train, y_train
                )

                # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«é¸æŠ
                best_model_name = max(
                    model_scores.keys(), key=lambda x: model_scores[x]["accuracy"]
                )
                best_model = trained_models[best_model_name]

                # ãƒ†ã‚¹ãƒˆäºˆæ¸¬
                X_test_scaled = self.scaler.transform(X_test)
                test_predictions = best_model.predict(X_test_scaled)

                # ãƒ†ã‚¹ãƒˆç²¾åº¦
                test_accuracy = accuracy_score(y_test, test_predictions)

                # è©³ç´°åˆ†æ
                test_class_counts = y_test.value_counts()
                print(f"  ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹åˆ†å¸ƒ: {dict(test_class_counts)}")

                result = {
                    "symbol": symbol,
                    "accuracy": test_accuracy,
                    "best_model": best_model_name,
                    "test_samples": len(X_test),
                    "train_accuracy": model_scores[best_model_name]["accuracy"],
                }

                all_results.append(result)

                print(f"  ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«: {best_model_name}")
                print(f"  ãƒ†ã‚¹ãƒˆç²¾åº¦: {test_accuracy:.3f}")

                if test_accuracy >= 0.8:
                    print("  âœ“ 80%ä»¥ä¸Šé”æˆï¼")
                elif test_accuracy >= 0.75:
                    print("  â–³ 75%ä»¥ä¸Š")

            except Exception as e:
                print(f"  ã‚¨ãƒ©ãƒ¼: {str(e)}")
                continue

        return self._analyze_directional_results(all_results)

    def _analyze_directional_results(self, results: List[Dict]) -> Dict:
        """æ–¹å‘æ€§äºˆæ¸¬çµæœã®åˆ†æ"""
        if not results:
            return {"error": "No results"}

        accuracies = [r["accuracy"] for r in results]

        max_accuracy = np.max(accuracies)
        avg_accuracy = np.mean(accuracies)
        median_accuracy = np.median(accuracies)

        # ç²¾åº¦åˆ¥ã‚«ã‚¦ãƒ³ãƒˆ
        high_accuracy_count = sum(1 for acc in accuracies if acc >= 0.8)
        good_accuracy_count = sum(1 for acc in accuracies if acc >= 0.75)

        print(f"\n" + "=" * 60)
        print("æ–¹å‘æ€§äºˆæ¸¬çµæœåˆ†æ")
        print("=" * 60)
        print(f"ãƒ†ã‚¹ãƒˆéŠ˜æŸ„æ•°: {len(results)}")
        print(f"æœ€é«˜ç²¾åº¦: {max_accuracy:.1%}")
        print(f"å¹³å‡ç²¾åº¦: {avg_accuracy:.1%}")
        print(f"ä¸­å¤®å€¤ç²¾åº¦: {median_accuracy:.1%}")
        print(f"80%ä»¥ä¸Š: {high_accuracy_count}/{len(results)} éŠ˜æŸ„")
        print(f"75%ä»¥ä¸Š: {good_accuracy_count}/{len(results)} éŠ˜æŸ„")

        # æœ€å„ªç§€éŠ˜æŸ„
        best_result = max(results, key=lambda x: x["accuracy"])
        print(f"\næœ€å„ªç§€çµæœ:")
        print(f"  éŠ˜æŸ„: {best_result['symbol']}")
        print(f"  ç²¾åº¦: {best_result['accuracy']:.1%}")
        print(f"  ãƒ¢ãƒ‡ãƒ«: {best_result['best_model']}")

        if max_accuracy >= 0.8:
            print(f"\nğŸ‰ ç›®æ¨™é”æˆï¼æœ€é«˜ç²¾åº¦ {max_accuracy:.1%}")
        elif avg_accuracy >= 0.75:
            print(f"\nâ–³ è‰¯å¥½ãªçµæœï¼šå¹³å‡ç²¾åº¦ {avg_accuracy:.1%}")

        return {
            "max_accuracy": max_accuracy,
            "avg_accuracy": avg_accuracy,
            "high_accuracy_count": high_accuracy_count,
            "results": results,
        }


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("é«˜ç²¾åº¦æ–¹å‘æ€§äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ï¼ˆ80%ä»¥ä¸Šç›®æ¨™ï¼‰")
    print("=" * 60)

    data_provider = StockDataProvider()
    symbols = list(data_provider.jp_stock_codes.keys())

    predictor = AdvancedDirectionalPredictor()
    results = predictor.test_directional_system(symbols)

    if "error" not in results:
        print(f"\næœ€çµ‚è©•ä¾¡: æœ€é«˜ç²¾åº¦ {results['max_accuracy']:.1%}")
        if results["max_accuracy"] >= 0.8:
            print("âœ“ 80%ä»¥ä¸Šã®æ–¹å‘æ€§äºˆæ¸¬ç²¾åº¦ã‚’é”æˆï¼")


if __name__ == "__main__":
    main()
