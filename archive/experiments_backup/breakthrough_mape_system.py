#!/usr/bin/env python3
"""
ChatGPTãŒè¨€åŠã—ãŸ10-20% MAPEé”æˆã®ãŸã‚ã®æ ¹æœ¬çš„æ–°ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
å•é¡Œã®æ ¸å¿ƒï¼šå¾“æ¥ã®æ‰‹æ³•ã®æ ¹æœ¬çš„è¦‹ç›´ã—
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple
import logging
from sklearn.ensemble import GradientBoostingRegressor, ExtraTreesRegressor
from sklearn.linear_model import ElasticNet, HuberRegressor
from sklearn.preprocessing import RobustScaler
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_absolute_error
import warnings

warnings.filterwarnings("ignore")

from data.stock_data import StockDataProvider

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class BreakthroughMAPESystem:
    """ChatGPTç†è«–ã«åŸºã¥ã10-20% MAPEé”æˆã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        self.data_provider = StockDataProvider()
        self.scaler = RobustScaler()

    def create_intelligent_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """ChatGPTç†è«–ã«åŸºã¥ãçŸ¥çš„ç‰¹å¾´é‡ä½œæˆ"""
        features = pd.DataFrame(index=data.index)

        close = data["Close"]
        volume = data["Volume"]
        high = data["High"]
        low = data["Low"]

        # 1. æ–¹å‘æ€§é‡è¦–ã®ç‰¹å¾´é‡ï¼ˆåˆ†é¡çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼‰
        returns_1d = close.pct_change()
        returns_5d = close.pct_change(5)

        # å¼·ã„ãƒˆãƒ¬ãƒ³ãƒ‰ã®æ¤œå‡º
        sma_5 = close.rolling(5).mean()
        sma_20 = close.rolling(20).mean()

        # ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦ï¼ˆé‡è¦ï¼‰
        trend_strength = (sma_5 - sma_20) / sma_20
        features["trend_strength"] = trend_strength
        features["trend_consistency"] = trend_strength.rolling(5).std()

        # ä¾¡æ ¼ä½ç½®ï¼ˆãƒ¬ãƒ³ã‚¸å†…ã§ã®ä½ç½®ï¼‰
        for window in [10, 20]:
            rolling_max = high.rolling(window).max()
            rolling_min = low.rolling(window).min()
            price_position = (close - rolling_min) / (rolling_max - rolling_min)
            features[f"price_position_{window}"] = price_position

        # 2. ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æ­£è¦åŒ–ã•ã‚ŒãŸç‰¹å¾´é‡
        vol_20 = returns_1d.rolling(20).std()
        features["volatility_20"] = vol_20

        # æ­£è¦åŒ–ãƒªã‚¿ãƒ¼ãƒ³ï¼ˆé‡è¦ï¼‰
        features["normalized_return_1d"] = returns_1d / vol_20
        features["normalized_return_5d"] = returns_5d / vol_20

        # 3. å¹³å‡å›å¸°ã‚·ã‚°ãƒŠãƒ«
        mean_return_20 = returns_1d.rolling(20).mean()
        features["mean_reversion_signal"] = (returns_1d - mean_return_20) / vol_20

        # 4. å‡ºæ¥é«˜ã‚·ã‚°ãƒŠãƒ«
        volume_sma = volume.rolling(20).mean()
        volume_ratio = volume / volume_sma
        features["volume_signal"] = volume_ratio
        features["volume_price_divergence"] = (volume_ratio - 1) * returns_1d

        # 5. RSIæ”¹è‰¯ç‰ˆï¼ˆæ­£è¦åŒ–ï¼‰
        rsi_14 = self._calculate_rsi(close, 14)
        features["rsi_normalized"] = (rsi_14 - 50) / 50  # -1 to 1 range
        features["rsi_extremes"] = np.where(
            rsi_14 > 70, 1, np.where(rsi_14 < 30, -1, 0)
        )

        # 6. MACDæ”¹è‰¯ç‰ˆ
        ema_12 = close.ewm(span=12).mean()
        ema_26 = close.ewm(span=26).mean()
        macd = ema_12 - ema_26
        macd_signal = macd.ewm(span=9).mean()
        features["macd_normalized"] = macd / close
        features["macd_divergence"] = (macd - macd_signal) / close

        # 7. å¸‚å ´çŠ¶æ³ç‰¹å¾´é‡
        features["market_stress"] = (
            vol_20 / vol_20.rolling(60).mean()
        )  # ç›¸å¯¾ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£

        # 8. ãƒ©ã‚°ç‰¹å¾´é‡ï¼ˆçŸ­æœŸè¨˜æ†¶ï¼‰
        for lag in [1, 2, 3]:
            features[f"return_lag_{lag}"] = returns_1d.shift(lag)
            features[f"volume_signal_lag_{lag}"] = features["volume_signal"].shift(lag)

        return features

    def _calculate_rsi(self, prices: pd.Series, window: int = 14) -> pd.Series:
        """RSIè¨ˆç®—"""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
        rs = gain / loss
        return 100 - (100 / (1 + rs))

    def create_target_variables(
        self, data: pd.DataFrame, prediction_days: int = 7
    ) -> pd.Series:
        """æ”¹è‰¯ã•ã‚ŒãŸã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ä½œæˆ"""
        close = data["Close"]

        # è¤‡æ•°æ—¥ã®å¹³å‡ãƒªã‚¿ãƒ¼ãƒ³ã‚’ä½¿ç”¨ï¼ˆãƒã‚¤ã‚ºå‰Šæ¸›ï¼‰
        future_returns = []
        for i in range(1, prediction_days + 1):
            daily_return = close.shift(-i).pct_change()
            future_returns.append(daily_return)

        # å¹³å‡ãƒªã‚¿ãƒ¼ãƒ³ï¼ˆã‚ˆã‚Šå®‰å®šï¼‰
        avg_future_return = pd.concat(future_returns, axis=1).mean(axis=1)

        return avg_future_return

    def preprocess_data(
        self, features: pd.DataFrame, target: pd.Series
    ) -> Tuple[pd.DataFrame, pd.Series]:
        """é«˜åº¦ãªå‰å‡¦ç†"""
        # 1. æ¬ æå€¤å‡¦ç†
        features = features.fillna(method="ffill").fillna(0)

        # 2. ç•°å¸¸å€¤å‡¦ç†ï¼ˆWinsorizingï¼‰
        for col in features.select_dtypes(include=[np.number]).columns:
            q01 = features[col].quantile(0.01)
            q99 = features[col].quantile(0.99)
            features[col] = np.clip(features[col], q01, q99)

        # 3. ç„¡é™å€¤å‡¦ç†
        features = features.replace([np.inf, -np.inf], np.nan).fillna(0)

        # 4. æœ‰åŠ¹ãªã‚µãƒ³ãƒ—ãƒ«ã®ã¿é¸æŠ
        valid_idx = ~(features.isna().any(axis=1) | target.isna())
        features_clean = features[valid_idx]
        target_clean = target[valid_idx]

        return features_clean, target_clean

    def train_ensemble_models(self, X: pd.DataFrame, y: pd.Series) -> Dict:
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°"""
        models = {
            "gradient_boosting": GradientBoostingRegressor(
                n_estimators=200,
                max_depth=6,
                learning_rate=0.1,
                subsample=0.8,
                random_state=42,
            ),
            "extra_trees": ExtraTreesRegressor(
                n_estimators=200, max_depth=8, random_state=42
            ),
            "huber": HuberRegressor(epsilon=1.35, max_iter=200),
        }

        # æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        tscv = TimeSeriesSplit(n_splits=5)
        model_scores = {}
        trained_models = {}

        for name, model in models.items():
            scores = []
            fold_predictions = []
            fold_actuals = []

            for train_idx, val_idx in tscv.split(X):
                X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
                y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

                # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
                X_train_scaled = self.scaler.fit_transform(X_train)
                X_val_scaled = self.scaler.transform(X_val)

                # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
                model.fit(X_train_scaled, y_train)

                # äºˆæ¸¬
                y_pred = model.predict(X_val_scaled)

                # MAPEè¨ˆç®—ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
                mape = self.calculate_robust_mape(y_val, y_pred)
                scores.append(mape)

                fold_predictions.extend(y_pred)
                fold_actuals.extend(y_val)

            avg_mape = np.mean(scores)
            model_scores[name] = {
                "mape": avg_mape,
                "std": np.std(scores),
                "predictions": fold_predictions,
                "actuals": fold_actuals,
            }

            # æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã‚’å…¨ãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´
            X_scaled = self.scaler.fit_transform(X)
            model.fit(X_scaled, y)
            trained_models[name] = model

            print(f"{name}: MAPE {avg_mape:.2f}% Â± {np.std(scores):.2f}%")

        return model_scores, trained_models

    def calculate_robust_mape(
        self, actual: pd.Series, predicted: pd.Series, threshold: float = 0.005
    ) -> float:
        """ãƒ­ãƒã‚¹ãƒˆMAPEè¨ˆç®—ï¼ˆå°ã•ãªãƒªã‚¿ãƒ¼ãƒ³ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼‰"""
        actual_arr = np.array(actual)
        predicted_arr = np.array(predicted)

        # é–¾å€¤ä»¥ä¸Šã®å‹•ãã®ã¿è©•ä¾¡
        mask = np.abs(actual_arr) >= threshold

        if mask.sum() < 5:  # æœ€ä½5ã‚µãƒ³ãƒ—ãƒ«å¿…è¦
            return float("inf")

        filtered_actual = actual_arr[mask]
        filtered_predicted = predicted_arr[mask]

        mape = (
            np.mean(np.abs((filtered_actual - filtered_predicted) / filtered_actual))
            * 100
        )
        return mape

    def test_breakthrough_system(self, symbols: List[str]) -> Dict:
        """çªç ´ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆ"""
        print("ChatGPTç†è«–ã«ã‚ˆã‚‹10-20% MAPEé”æˆãƒãƒ£ãƒ¬ãƒ³ã‚¸")
        print("=" * 60)

        all_predictions = []
        all_actuals = []
        symbol_results = {}

        for symbol in symbols[:10]:  # ã‚ˆã‚Šå¤šãã®éŠ˜æŸ„ã§ãƒ†ã‚¹ãƒˆ
            try:
                print(f"\nå‡¦ç†ä¸­: {symbol}")

                # ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆã‚ˆã‚Šé•·æœŸé–“ï¼‰
                data = self.data_provider.get_stock_data(symbol, "2y")
                if len(data) < 100:
                    continue

                # ç‰¹å¾´é‡ä½œæˆ
                features = self.create_intelligent_features(data)
                target = self.create_target_variables(data, prediction_days=7)

                # å‰å‡¦ç†
                features_clean, target_clean = self.preprocess_data(features, target)

                if len(features_clean) < 50:
                    continue

                print(f"  æœ‰åŠ¹ã‚µãƒ³ãƒ—ãƒ«: {len(features_clean)}")

                # å­¦ç¿’ãƒ»è©•ä¾¡ï¼ˆæœ€æ–°50%ã‚’ãƒ†ã‚¹ãƒˆç”¨ï¼‰
                split_point = int(len(features_clean) * 0.5)
                X_train = features_clean.iloc[:split_point]
                y_train = target_clean.iloc[:split_point]
                X_test = features_clean.iloc[split_point:]
                y_test = target_clean.iloc[split_point:]

                if len(X_test) < 20:
                    continue

                # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
                model_scores, trained_models = self.train_ensemble_models(
                    X_train, y_train
                )

                # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«é¸æŠ
                best_model_name = min(
                    model_scores.keys(), key=lambda x: model_scores[x]["mape"]
                )
                best_model = trained_models[best_model_name]

                # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§æœ€çµ‚è©•ä¾¡
                X_test_scaled = self.scaler.transform(X_test)
                test_predictions = best_model.predict(X_test_scaled)

                test_mape = self.calculate_robust_mape(y_test, test_predictions)

                symbol_results[symbol] = {
                    "mape": test_mape,
                    "best_model": best_model_name,
                    "test_samples": len(X_test),
                }

                all_predictions.extend(test_predictions)
                all_actuals.extend(y_test)

                print(f"  ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«: {best_model_name}")
                print(f"  ãƒ†ã‚¹ãƒˆMAPE: {test_mape:.2f}%")

                if test_mape < 20:
                    print("  âœ“ ç›®æ¨™ç¯„å›²é”æˆï¼")

            except Exception as e:
                logger.warning(f"Error processing {symbol}: {str(e)}")
                continue

        # å…¨ä½“çµæœ
        if all_predictions:
            overall_mape = self.calculate_robust_mape(
                pd.Series(all_actuals), pd.Series(all_predictions)
            )

            print(f"\n" + "=" * 60)
            print("æœ€çµ‚çµæœ")
            print("=" * 60)
            print(f"å…¨ä½“MAPE: {overall_mape:.2f}%")
            print(f"ç·ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«: {len(all_predictions)}")

            # éŠ˜æŸ„åˆ¥çµæœ
            successful_symbols = [
                s for s, r in symbol_results.items() if r["mape"] < 20
            ]
            if successful_symbols:
                print(f"\næˆåŠŸéŠ˜æŸ„ (MAPE < 20%): {len(successful_symbols)}éŠ˜æŸ„")
                for symbol in successful_symbols:
                    result = symbol_results[symbol]
                    print(f"  {symbol}: {result['mape']:.2f}% ({result['best_model']})")

            if overall_mape < 20:
                print(f"\nğŸ‰ ChatGPTç†è«–å®Ÿè¨¼æˆåŠŸï¼ MAPE {overall_mape:.2f}%")
            elif overall_mape < 30:
                print(f"\nâ–³ å¤§å¹…æ”¹å–„ï¼ç›®æ¨™ã¾ã§æ®‹ã‚Š{overall_mape - 20:.1f}%")
            else:
                print(f"\nç¶™ç¶šæ”¹å–„ãŒå¿…è¦ã€‚ç¾åœ¨{overall_mape:.2f}%")

            return {
                "overall_mape": overall_mape,
                "symbol_results": symbol_results,
                "total_samples": len(all_predictions),
                "successful_symbols": len(successful_symbols),
            }

        return {"error": "No valid results"}


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("ChatGPTç†è«–ã«ã‚ˆã‚‹10-20% MAPEé”æˆã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 60)

    data_provider = StockDataProvider()
    symbols = list(data_provider.jp_stock_codes.keys())

    system = BreakthroughMAPESystem()
    results = system.test_breakthrough_system(symbols)

    if "error" not in results:
        print(f"\næœ€çµ‚è©•ä¾¡:")
        print(f"ç›®æ¨™MAPE 10-20%ã«å¯¾ã—ã¦å®Ÿç¸¾{results['overall_mape']:.2f}%")
        if results["overall_mape"] <= 20:
            print("âœ“ ChatGPTç†è«–ã®æ­£å½“æ€§ç¢ºèªï¼")
        else:
            print("â–³ ã•ã‚‰ãªã‚‹æ”¹å–„ãŒå¿…è¦")


if __name__ == "__main__":
    main()
