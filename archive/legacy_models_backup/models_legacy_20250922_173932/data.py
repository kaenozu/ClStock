"""Data providers and analysis modules."""

import logging
from datetime import datetime, timedelta
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd

logger = logging.getLogger(__name__)


class MacroEconomicDataProvider:
    """ãƒã‚¯ãƒ­çµŒæ¸ˆãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼"""

    def __init__(self):
        self.data_cache: Dict[str, pd.DataFrame] = {}
        self.cache_expiry = timedelta(hours=6)  # 6æ™‚é–“ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœŸé™åˆ‡ã‚Œ

    def get_economic_indicators(self, start_date: str, end_date: str) -> pd.DataFrame:
        """çµŒæ¸ˆæŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        cache_key = f"economic_{start_date}_{end_date}"

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        if cache_key in self.data_cache:
            cached_data, cached_time = self.data_cache[cache_key]
            if datetime.now() - cached_time < self.cache_expiry:
                return cached_data

        try:
            # å®Ÿéš›ã®APIã‚³ãƒ¼ãƒ«ã®ä»£ã‚ã‚Šã«ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
            economic_data = self._generate_dummy_economic_data(start_date, end_date)

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            self.data_cache[cache_key] = (economic_data, datetime.now())

            return economic_data

        except Exception as e:
            logger.error(f"Economic data fetch failed: {e}")
            return pd.DataFrame()

    def _generate_dummy_economic_data(
        self,
        start_date: str,
        end_date: str,
    ) -> pd.DataFrame:
        """ãƒ€ãƒŸãƒ¼ã®çµŒæ¸ˆãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"""
        dates = pd.date_range(start=start_date, end=end_date, freq="D")

        # çµŒæ¸ˆæŒ‡æ¨™ã®ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
        np.random.seed(42)
        data = {
            "gdp_growth": np.random.normal(2.0, 0.5, len(dates)),
            "inflation_rate": np.random.normal(2.5, 0.3, len(dates)),
            "unemployment_rate": np.random.normal(4.0, 0.2, len(dates)),
            "interest_rate": np.random.normal(1.5, 0.1, len(dates)),
            "consumer_confidence": np.random.normal(100, 10, len(dates)),
            "manufacturing_pmi": np.random.normal(52, 3, len(dates)),
            "services_pmi": np.random.normal(53, 3, len(dates)),
            "dollar_index": np.random.normal(95, 2, len(dates)),
            "vix_index": np.random.normal(20, 5, len(dates)),
            "oil_price": np.random.normal(70, 10, len(dates)),
        }

        return pd.DataFrame(data, index=dates)

    def get_sector_performance(self, symbols: List[str]) -> Dict[str, float]:
        """ã‚»ã‚¯ã‚¿ãƒ¼åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å–å¾—"""
        # ã‚»ã‚¯ã‚¿ãƒ¼åˆ†é¡ã®ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
        sector_mapping = {
            "AAPL": "Technology",
            "GOOGL": "Technology",
            "MSFT": "Technology",
            "JPM": "Financial",
            "BAC": "Financial",
            "JNJ": "Healthcare",
            "PFE": "Healthcare",
            "XOM": "Energy",
            "CVX": "Energy",
        }

        sector_performance = {}
        for symbol in symbols:
            sector = sector_mapping.get(symbol, "Unknown")
            if sector not in sector_performance:
                # ãƒ©ãƒ³ãƒ€ãƒ ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç”Ÿæˆ
                sector_performance[sector] = np.random.normal(0.02, 0.05)

        return sector_performance

    def get_market_sentiment_indicators(self) -> Dict[str, float]:
        """å¸‚å ´ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆæŒ‡æ¨™ã‚’å–å¾—"""
        return {
            "fear_greed_index": np.random.uniform(0, 100),
            "put_call_ratio": np.random.uniform(0.5, 1.5),
            "margin_debt": np.random.uniform(-0.1, 0.1),
            "insider_trading_ratio": np.random.uniform(-0.05, 0.05),
            "short_interest": np.random.uniform(0.1, 0.3),
        }


class SentimentAnalyzer:
    """ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æå™¨"""

    def __init__(self):
        self.sentiment_cache: Dict[str, Tuple[float, datetime]] = {}
        self.cache_expiry = timedelta(hours=1)  # 1æ™‚é–“ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœŸé™åˆ‡ã‚Œ

    def analyze_news_sentiment(self, symbol: str, articles: List[str]) -> float:
        """ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã®ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æ"""
        cache_key = f"news_{symbol}_{hash(str(articles))}"

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        if cache_key in self.sentiment_cache:
            sentiment, cached_time = self.sentiment_cache[cache_key]
            if datetime.now() - cached_time < self.cache_expiry:
                return sentiment

        try:
            # å®Ÿéš›ã®NLPå‡¦ç†ã®ä»£ã‚ã‚Šã«ãƒ€ãƒŸãƒ¼åˆ†æ
            sentiment_score = self._dummy_sentiment_analysis(articles)

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            self.sentiment_cache[cache_key] = (sentiment_score, datetime.now())

            return sentiment_score

        except Exception as e:
            logger.error(f"Sentiment analysis failed for {symbol}: {e}")
            return 0.0  # ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«

    def _dummy_sentiment_analysis(self, articles: List[str]) -> float:
        """ãƒ€ãƒŸãƒ¼ã®ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æ"""
        if not articles:
            return 0.0

        # ç°¡å˜ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®åˆ†æ
        positive_words = ["growth", "profit", "increase", "bull", "upgrade", "strong"]
        negative_words = ["loss", "decline", "bear", "downgrade", "weak", "crisis"]

        total_sentiment = 0.0
        for article in articles:
            article_lower = article.lower()

            positive_count = sum(1 for word in positive_words if word in article_lower)
            negative_count = sum(1 for word in negative_words if word in article_lower)

            # ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆã‚¹ã‚³ã‚¢è¨ˆç®— (-1 to 1)
            if positive_count + negative_count > 0:
                article_sentiment = (positive_count - negative_count) / (
                    positive_count + negative_count
                )
            else:
                article_sentiment = 0.0

            total_sentiment += article_sentiment

        # å¹³å‡ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ
        avg_sentiment = total_sentiment / len(articles) if articles else 0.0
        return np.clip(avg_sentiment, -1.0, 1.0)

    def analyze_social_media_sentiment(self, symbol: str, posts: List[str]) -> float:
        """ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒ¡ãƒ‡ã‚£ã‚¢æŠ•ç¨¿ã®ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æ"""
        cache_key = f"social_{symbol}_{hash(str(posts))}"

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        if cache_key in self.sentiment_cache:
            sentiment, cached_time = self.sentiment_cache[cache_key]
            if datetime.now() - cached_time < self.cache_expiry:
                return sentiment

        try:
            # ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒ¡ãƒ‡ã‚£ã‚¢ç‰¹æœ‰ã®åˆ†æ
            sentiment_score = self._analyze_social_posts(posts)

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            self.sentiment_cache[cache_key] = (sentiment_score, datetime.now())

            return sentiment_score

        except Exception as e:
            logger.error(f"Social media sentiment analysis failed for {symbol}: {e}")
            return 0.0

    def _analyze_social_posts(self, posts: List[str]) -> float:
        """ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒ¡ãƒ‡ã‚£ã‚¢æŠ•ç¨¿ã®åˆ†æ"""
        if not posts:
            return 0.0

        # ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒ¡ãƒ‡ã‚£ã‚¢ç‰¹æœ‰ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        positive_emojis = ["ğŸš€", "ğŸ“ˆ", "ğŸ’", "ğŸ”¥", "ğŸ‘", "âœ¨"]
        negative_emojis = ["ğŸ“‰", "ğŸ˜­", "ğŸ’€", "ğŸ‘", "ğŸ˜°", "ğŸ”»"]

        bullish_phrases = ["to the moon", "diamond hands", "buy the dip", "hodl"]
        bearish_phrases = ["paper hands", "dump it", "sell off", "crash"]

        total_sentiment = 0.0
        for post in posts:
            post_lower = post.lower()

            # çµµæ–‡å­—ã‚«ã‚¦ãƒ³ãƒˆ
            positive_emoji_count = sum(1 for emoji in positive_emojis if emoji in post)
            negative_emoji_count = sum(1 for emoji in negative_emojis if emoji in post)

            # ãƒ•ãƒ¬ãƒ¼ã‚ºã‚«ã‚¦ãƒ³ãƒˆ
            bullish_count = sum(1 for phrase in bullish_phrases if phrase in post_lower)
            bearish_count = sum(1 for phrase in bearish_phrases if phrase in post_lower)

            # ç·åˆã‚«ã‚¦ãƒ³ãƒˆ
            positive_total = positive_emoji_count + bullish_count
            negative_total = negative_emoji_count + bearish_count

            if positive_total + negative_total > 0:
                post_sentiment = (positive_total - negative_total) / (
                    positive_total + negative_total
                )
            else:
                post_sentiment = 0.0

            total_sentiment += post_sentiment

        avg_sentiment = total_sentiment / len(posts) if posts else 0.0
        return np.clip(avg_sentiment, -1.0, 1.0)

    def get_sentiment_summary(
        self,
        symbol: str,
        timeframe: str = "1d",
    ) -> Dict[str, float]:
        """ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆã‚µãƒãƒªãƒ¼ã‚’å–å¾—"""
        # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
        base_sentiment = np.random.normal(0, 0.3)

        return {
            "overall_sentiment": np.clip(base_sentiment, -1.0, 1.0),
            "news_sentiment": np.clip(
                base_sentiment + np.random.normal(0, 0.1),
                -1.0,
                1.0,
            ),
            "social_sentiment": np.clip(
                base_sentiment + np.random.normal(0, 0.2),
                -1.0,
                1.0,
            ),
            "analyst_sentiment": np.clip(
                base_sentiment + np.random.normal(0, 0.15),
                -1.0,
                1.0,
            ),
            "sentiment_momentum": np.random.normal(0, 0.1),
            "sentiment_volume": np.random.uniform(0.1, 1.0),
        }

    def calculate_sentiment_impact(
        self,
        current_sentiment: float,
        historical_sentiment: List[float],
    ) -> float:
        """ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆã®å½±éŸ¿åº¦ã‚’è¨ˆç®—"""
        if not historical_sentiment:
            return 0.0

        # ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆã®å¤‰åŒ–ã‚’è¨ˆç®—
        avg_historical = np.mean(historical_sentiment)
        sentiment_change = current_sentiment - avg_historical

        # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚’è€ƒæ…®
        sentiment_volatility = (
            np.std(historical_sentiment) if len(historical_sentiment) > 1 else 0.1
        )

        # å½±éŸ¿åº¦è¨ˆç®—ï¼ˆæ¨™æº–åŒ–ï¼‰
        impact = sentiment_change / (sentiment_volatility + 0.01)

        return np.clip(impact, -3.0, 3.0)  # -3ã‹ã‚‰3ã®ç¯„å›²ã«åˆ¶é™

    def clear_cache(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        self.sentiment_cache.clear()

    def get_cache_stats(self) -> Dict[str, int]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆã‚’å–å¾—"""
        now = datetime.now()
        active_entries = sum(
            1
            for _, cached_time in self.sentiment_cache.values()
            if now - cached_time < self.cache_expiry
        )

        return {
            "total_cached_items": len(self.sentiment_cache),
            "active_cached_items": active_entries,
            "expired_items": len(self.sentiment_cache) - active_entries,
        }
