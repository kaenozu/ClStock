#!/usr/bin/env python3
"""ClStock é«˜åº¦ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ãƒ†ã‚¹ãƒˆã‚·ã‚¹ãƒ†ãƒ 
84.6%ç²¾åº¦çªç ´ã‚’ç›®æŒ‡ã™çµ±åˆæ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ 

å®Ÿè£…æ©Ÿèƒ½:
- BERTæ´»ç”¨ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æ
- ãƒã‚¯ãƒ­çµŒæ¸ˆæŒ‡æ¨™çµ±åˆäºˆæ¸¬
- å‹•çš„é‡ã¿èª¿æ•´ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’
- æ·±å±¤å­¦ç¿’æœ€é©åŒ– (LSTM/Transformer)
"""

import logging
import sys
import warnings
from datetime import datetime
from typing import Any, Dict, List

import numpy as np
from utils.logger_config import setup_logger

warnings.filterwarnings("ignore")

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
)
logger = setup_logger(__name__)


class AdvancedEnsembleTestSystem:
    """é«˜åº¦ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ãƒ†ã‚¹ãƒˆã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        self.test_symbols = [
            "7203",
            "6758",
            "9984",
            "8306",
            "6861",
            "4661",
            "9433",
            "4519",
            "6367",
            "8035",
        ]
        self.results = {}
        self.ensemble_predictor = None
        self.macro_provider = None

    def initialize_systems(self):
        """ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        try:
            logger.info("=== é«˜åº¦ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ– ===")

            # models/ml_models.py ã‹ã‚‰é«˜åº¦ã‚¯ãƒ©ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
            sys.path.append(".")
            from models.ml_models import (
                AdvancedEnsemblePredictor,
                MacroEconomicDataProvider,
            )

            self.ensemble_predictor = AdvancedEnsemblePredictor()
            self.macro_provider = MacroEconomicDataProvider()

            # å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–
            self.ensemble_predictor.initialize_components()

            logger.info("ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
            return True

        except Exception as e:
            logger.error(f"ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def run_comprehensive_test(self) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        logger.info("\n" + "=" * 60)
        logger.info("ClStock é«˜åº¦ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ åŒ…æ‹¬ãƒ†ã‚¹ãƒˆé–‹å§‹")
        logger.info("ç›®æ¨™: 84.6%ç²¾åº¦çªç ´")
        logger.info("=" * 60)

        test_results = {
            "system_info": self._get_system_info(),
            "macro_data_test": self._test_macro_data_integration(),
            "sentiment_analysis_test": self._test_sentiment_analysis(),
            "deep_learning_test": self._test_deep_learning_models(),
            "ensemble_prediction_test": self._test_ensemble_predictions(),
            "accuracy_benchmark": self._run_accuracy_benchmark(),
            "performance_summary": {},
        }

        # ç·åˆè©•ä¾¡
        test_results["performance_summary"] = self._generate_performance_summary(
            test_results,
        )

        return test_results

    def _get_system_info(self) -> Dict[str, Any]:
        """ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±å–å¾—"""
        import platform

        import psutil

        return {
            "timestamp": datetime.now().isoformat(),
            "python_version": platform.python_version(),
            "platform": platform.platform(),
            "cpu_count": psutil.cpu_count(),
            "memory_gb": round(psutil.virtual_memory().total / (1024**3), 2),
            "test_symbols": self.test_symbols,
            "test_count": len(self.test_symbols),
        }

    def _test_macro_data_integration(self) -> Dict[str, Any]:
        """ãƒã‚¯ãƒ­çµŒæ¸ˆãƒ‡ãƒ¼ã‚¿çµ±åˆãƒ†ã‚¹ãƒˆ"""
        logger.info("\n--- ãƒã‚¯ãƒ­çµŒæ¸ˆãƒ‡ãƒ¼ã‚¿çµ±åˆãƒ†ã‚¹ãƒˆ ---")

        try:
            # ãƒã‚¯ãƒ­çµŒæ¸ˆæŒ‡æ¨™å–å¾—
            macro_data = self.macro_provider.get_economic_indicators()

            test_result = {
                "status": "success",
                "data_sources": list(macro_data.keys()),
                "boj_policy_available": "boj_policy" in macro_data,
                "global_rates_available": "global_rates" in macro_data,
                "currency_data_available": "currency_strength" in macro_data,
                "sentiment_indicators_available": "market_sentiment" in macro_data,
                "data_completeness": len(macro_data) / 4.0,  # 4ã¤ã®ä¸»è¦ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹
            }

            logger.info(f"ãƒã‚¯ãƒ­ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹: {test_result['data_sources']}")
            logger.info(f"ãƒ‡ãƒ¼ã‚¿å®Œå…¨æ€§: {test_result['data_completeness']:.1%}")

            return test_result

        except Exception as e:
            logger.error(f"ãƒã‚¯ãƒ­ãƒ‡ãƒ¼ã‚¿ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return {"status": "error", "error": str(e)}

    def _test_sentiment_analysis(self) -> Dict[str, Any]:
        """ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æãƒ†ã‚¹ãƒˆ"""
        logger.info("\n--- ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æãƒ†ã‚¹ãƒˆ ---")

        test_results = {}
        successful_analyses = 0

        for symbol in self.test_symbols[:5]:  # 5éŠ˜æŸ„ã§ãƒ†ã‚¹ãƒˆ
            try:
                sentiment_result = (
                    self.ensemble_predictor.enhanced_sentiment_prediction(symbol)
                )

                test_results[symbol] = {
                    "sentiment_score": sentiment_result["sentiment_score"],
                    "confidence": sentiment_result["confidence"],
                    "status": "success",
                }

                if abs(sentiment_result["sentiment_score"]) > 0.1:  # æœ‰æ„ãªã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ
                    successful_analyses += 1

                logger.info(
                    f"{symbol}: ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ={sentiment_result['sentiment_score']:.3f}, ä¿¡é ¼åº¦={sentiment_result['confidence']:.3f}",
                )

            except Exception as e:
                test_results[symbol] = {"status": "error", "error": str(e)}
                logger.error(f"{symbol}: ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æã‚¨ãƒ©ãƒ¼ - {e}")

        return {
            "individual_results": test_results,
            "success_rate": successful_analyses / len(self.test_symbols[:5]),
            "bert_available": hasattr(self.ensemble_predictor, "bert_model"),
            "total_tested": len(self.test_symbols[:5]),
        }

    def _test_deep_learning_models(self) -> Dict[str, Any]:
        """æ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆ"""
        logger.info("\n--- æ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆ ---")

        try:
            # ç°¡æ˜“è¨“ç·´ï¼ˆè¨ˆç®—é‡åˆ¶é™ï¼‰
            train_symbols = self.test_symbols[:3]
            logger.info(f"æ·±å±¤å­¦ç¿’è¨“ç·´é–‹å§‹: {train_symbols}")

            # æ—¢å­˜ã®æ·±å±¤å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ
            from experiments.big_data_deep_learning import BigDataDeepLearningSystem

            dl_system = BigDataDeepLearningSystem()
            results = dl_system.test_multiple_symbols(train_symbols)

            # çµæœè§£æ
            best_accuracies = []
            for symbol, result in results.items():
                if "best_accuracy" in result:
                    best_accuracies.append(result["best_accuracy"])

            avg_accuracy = np.mean(best_accuracies) if best_accuracies else 0.0
            max_accuracy = np.max(best_accuracies) if best_accuracies else 0.0

            return {
                "status": "success",
                "tested_symbols": train_symbols,
                "average_accuracy": avg_accuracy,
                "max_accuracy": max_accuracy,
                "models_tested": ["Advanced LSTM", "Advanced GRU", "CNN-LSTM Hybrid"],
                "accuracy_above_60": sum(1 for acc in best_accuracies if acc > 60.0),
                "individual_results": results,
            }

        except Exception as e:
            logger.error(f"æ·±å±¤å­¦ç¿’ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return {"status": "error", "error": str(e)}

    def _test_ensemble_predictions(self) -> Dict[str, Any]:
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬ãƒ†ã‚¹ãƒˆ"""
        logger.info("\n--- ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬ãƒ†ã‚¹ãƒˆ ---")

        ensemble_results = {}
        high_confidence_predictions = 0
        total_tested = 0

        for symbol in self.test_symbols:
            try:
                result = self.ensemble_predictor.dynamic_ensemble_prediction(symbol)

                ensemble_results[symbol] = {
                    "ensemble_prediction": result["ensemble_prediction"],
                    "ensemble_confidence": result["ensemble_confidence"],
                    "high_confidence": result["high_confidence"],
                    "individual_predictions": result.get("individual_predictions", {}),
                    "adjusted_weights": result.get("adjusted_weights", {}),
                }

                if result["high_confidence"]:
                    high_confidence_predictions += 1

                total_tested += 1

                logger.info(
                    f"{symbol}: äºˆæ¸¬={result['ensemble_prediction']:.1f}, "
                    f"ä¿¡é ¼åº¦={result['ensemble_confidence']:.3f}, "
                    f"é«˜ä¿¡é ¼={result['high_confidence']}",
                )

            except Exception as e:
                ensemble_results[symbol] = {"status": "error", "error": str(e)}
                logger.error(f"{symbol}: ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬ã‚¨ãƒ©ãƒ¼ - {e}")

        return {
            "individual_results": ensemble_results,
            "high_confidence_rate": (
                high_confidence_predictions / total_tested if total_tested > 0 else 0
            ),
            "total_tested": total_tested,
            "high_confidence_count": high_confidence_predictions,
        }

    def _run_accuracy_benchmark(self) -> Dict[str, Any]:
        """ç²¾åº¦ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ"""
        logger.info("\n--- ç²¾åº¦ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ vs 84.6%åŸºæº– ---")

        try:
            # 84.6%ãƒ™ãƒ¼ã‚¹ã‚·ã‚¹ãƒ†ãƒ ã¨ã®æ¯”è¼ƒ
            from trend_following_predictor import TrendFollowingPredictor

            base_predictor = TrendFollowingPredictor()
            comparison_results = {}

            ensemble_wins = 0
            base_wins = 0
            ties = 0

            for symbol in self.test_symbols[:7]:  # è¨ˆç®—é‡åˆ¶é™
                try:
                    # ãƒ™ãƒ¼ã‚¹äºˆæ¸¬
                    base_result = base_predictor.predict_stock(symbol)
                    base_confidence = base_result["confidence"]

                    # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬
                    ensemble_result = (
                        self.ensemble_predictor.dynamic_ensemble_prediction(symbol)
                    )
                    ensemble_confidence = ensemble_result["ensemble_confidence"]

                    # ä¿¡é ¼åº¦æ¯”è¼ƒ
                    if ensemble_confidence > base_confidence + 0.05:  # 5%ä»¥ä¸Šã®æ”¹å–„
                        winner = "ensemble"
                        ensemble_wins += 1
                    elif base_confidence > ensemble_confidence + 0.05:
                        winner = "base"
                        base_wins += 1
                    else:
                        winner = "tie"
                        ties += 1

                    comparison_results[symbol] = {
                        "base_confidence": base_confidence,
                        "ensemble_confidence": ensemble_confidence,
                        "improvement": ensemble_confidence - base_confidence,
                        "winner": winner,
                    }

                    logger.info(
                        f"{symbol}: ãƒ™ãƒ¼ã‚¹={base_confidence:.3f}, "
                        f"ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«={ensemble_confidence:.3f}, "
                        f"æ”¹å–„={ensemble_confidence - base_confidence:+.3f}",
                    )

                except Exception as e:
                    logger.error(f"{symbol}: ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ - {e}")

            total_comparisons = ensemble_wins + base_wins + ties

            return {
                "ensemble_wins": ensemble_wins,
                "base_wins": base_wins,
                "ties": ties,
                "ensemble_win_rate": (
                    ensemble_wins / total_comparisons if total_comparisons > 0 else 0
                ),
                "average_improvement": np.mean(
                    [r.get("improvement", 0) for r in comparison_results.values()],
                ),
                "individual_comparisons": comparison_results,
                "total_comparisons": total_comparisons,
            }

        except Exception as e:
            logger.error(f"ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return {"status": "error", "error": str(e)}

    def _generate_performance_summary(
        self,
        test_results: Dict[str, Any],
    ) -> Dict[str, Any]:
        """ç·åˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡"""
        logger.info("\n" + "=" * 60)
        logger.info("ç·åˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡")
        logger.info("=" * 60)

        try:
            # å„ãƒ†ã‚¹ãƒˆé …ç›®ã®ã‚¹ã‚³ã‚¢ç®—å‡º
            scores = {}

            # ãƒã‚¯ãƒ­ãƒ‡ãƒ¼ã‚¿çµ±åˆã‚¹ã‚³ã‚¢
            macro_test = test_results.get("macro_data_test", {})
            scores["macro_integration"] = macro_test.get("data_completeness", 0) * 100

            # ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æã‚¹ã‚³ã‚¢
            sentiment_test = test_results.get("sentiment_analysis_test", {})
            scores["sentiment_analysis"] = sentiment_test.get("success_rate", 0) * 100

            # æ·±å±¤å­¦ç¿’ã‚¹ã‚³ã‚¢
            dl_test = test_results.get("deep_learning_test", {})
            scores["deep_learning"] = dl_test.get("max_accuracy", 0)

            # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬ã‚¹ã‚³ã‚¢
            ensemble_test = test_results.get("ensemble_prediction_test", {})
            scores["ensemble_prediction"] = (
                ensemble_test.get("high_confidence_rate", 0) * 100
            )

            # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚³ã‚¢
            benchmark_test = test_results.get("accuracy_benchmark", {})
            scores["benchmark_performance"] = (
                benchmark_test.get("ensemble_win_rate", 0) * 100
            )

            # ç·åˆã‚¹ã‚³ã‚¢ç®—å‡º
            weight_map = {
                "macro_integration": 0.15,
                "sentiment_analysis": 0.20,
                "deep_learning": 0.30,
                "ensemble_prediction": 0.20,
                "benchmark_performance": 0.15,
            }

            overall_score = sum(
                scores[key] * weight_map[key] for key in scores if key in weight_map
            )

            # 84.6%åŸºæº–ã¨ã®æ¯”è¼ƒ
            baseline_accuracy = 84.6
            projected_accuracy = (
                baseline_accuracy + (overall_score - 70) * 0.2
            )  # ã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹æ¨å®š

            # è©•ä¾¡ã‚°ãƒ¬ãƒ¼ãƒ‰
            if projected_accuracy >= 87.0:
                grade = "S (å“è¶Š)"
            elif projected_accuracy >= 85.5:
                grade = "A+ (å„ªç§€)"
            elif projected_accuracy >= 84.6:
                grade = "A (è‰¯å¥½)"
            elif projected_accuracy >= 82.0:
                grade = "B (æ¨™æº–)"
            else:
                grade = "C (è¦æ”¹å–„)"

            summary = {
                "individual_scores": scores,
                "overall_score": overall_score,
                "projected_accuracy": projected_accuracy,
                "baseline_accuracy": baseline_accuracy,
                "accuracy_improvement": projected_accuracy - baseline_accuracy,
                "grade": grade,
                "target_achieved": projected_accuracy >= 85.0,  # 85%ç›®æ¨™
                "recommendation": self._generate_recommendations(
                    scores,
                    projected_accuracy,
                ),
            }

            # çµæœè¡¨ç¤º
            logger.info("å€‹åˆ¥ã‚¹ã‚³ã‚¢:")
            for key, score in scores.items():
                logger.info(f"  {key}: {score:.1f}")

            logger.info("\nç·åˆè©•ä¾¡:")
            logger.info(f"  ç·åˆã‚¹ã‚³ã‚¢: {overall_score:.1f}/100")
            logger.info(
                f"  äºˆæ¸¬ç²¾åº¦: {projected_accuracy:.1f}% (åŸºæº–: {baseline_accuracy}%)",
            )
            logger.info(f"  ç²¾åº¦æ”¹å–„: {projected_accuracy - baseline_accuracy:+.1f}%")
            logger.info(f"  è©•ä¾¡ã‚°ãƒ¬ãƒ¼ãƒ‰: {grade}")
            logger.info(
                f"  ç›®æ¨™é”æˆ: {'âœ… YES' if summary['target_achieved'] else 'âŒ NO'}",
            )

            return summary

        except Exception as e:
            logger.error(f"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
            return {"status": "error", "error": str(e)}

    def _generate_recommendations(
        self,
        scores: Dict[str, float],
        projected_accuracy: float,
    ) -> List[str]:
        """æ”¹å–„æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        recommendations = []

        if scores.get("macro_integration", 0) < 70:
            recommendations.append("ãƒã‚¯ãƒ­çµŒæ¸ˆãƒ‡ãƒ¼ã‚¿çµ±åˆã®æ”¹å–„ãŒå¿…è¦")

        if scores.get("sentiment_analysis", 0) < 60:
            recommendations.append("ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æç²¾åº¦ã®å‘ä¸ŠãŒå¿…è¦")

        if scores.get("deep_learning", 0) < 65:
            recommendations.append("æ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®æœ€é©åŒ–ãŒå¿…è¦")

        if scores.get("ensemble_prediction", 0) < 75:
            recommendations.append("ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é‡ã¿èª¿æ•´ã®æ”¹å–„ãŒå¿…è¦")

        if projected_accuracy < 85.0:
            recommendations.append("è¿½åŠ ã®ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚’æ¨å¥¨")
            recommendations.append("ã‚ˆã‚Šé•·æœŸé–“ã®ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆãŒå¿…è¦")

        if not recommendations:
            recommendations.append("å„ªç§€ãªæ€§èƒ½ã§ã™ï¼æœ¬æ ¼é‹ç”¨ã‚’æ¤œè¨ã—ã¦ãã ã•ã„")

        return recommendations


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("=" * 70)
    print("ClStock é«˜åº¦ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ãƒ†ã‚¹ãƒˆã‚·ã‚¹ãƒ†ãƒ ")
    print("84.6%ç²¾åº¦çªç ´ã¸ã®æŒ‘æˆ¦")
    print("=" * 70)

    # ãƒ†ã‚¹ãƒˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    test_system = AdvancedEnsembleTestSystem()

    if not test_system.initialize_systems():
        print("âŒ ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return 1

    try:
        # åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        results = test_system.run_comprehensive_test()

        # çµæœä¿å­˜
        import json

        output_file = f"advanced_ensemble_test_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

        # JSON serializable ã«å¤‰æ›
        serializable_results = {}
        for key, value in results.items():
            try:
                json.dumps(value)  # ãƒ†ã‚¹ãƒˆ
                serializable_results[key] = value
            except:
                serializable_results[key] = str(value)

        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(serializable_results, f, indent=2, ensure_ascii=False)

        print(f"\nğŸ“Š è©³ç´°çµæœã‚’ä¿å­˜: {output_file}")

        # æœ€çµ‚åˆ¤å®š
        summary = results.get("performance_summary", {})
        if summary.get("target_achieved", False):
            print("\nğŸ‰ 85%ç²¾åº¦ç›®æ¨™é”æˆï¼")
            print(f"äºˆæ¸¬ç²¾åº¦: {summary.get('projected_accuracy', 0):.1f}%")
        else:
            print("\nğŸ“ˆ ç¶™ç¶šæ”¹å–„ãŒå¿…è¦")
            print(f"ç¾åœ¨äºˆæ¸¬ç²¾åº¦: {summary.get('projected_accuracy', 0):.1f}%")
            print("æ¨å¥¨æ”¹å–„äº‹é …:")
            for rec in summary.get("recommendation", []):
                print(f"  â€¢ {rec}")

        return 0

    except KeyboardInterrupt:
        print("\nâ¸ï¸  ãƒ†ã‚¹ãƒˆä¸­æ–­")
        return 130
    except Exception as e:
        logger.error(f"ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        return 1


if __name__ == "__main__":
    sys.exit(main())
