#!/usr/bin/env python3
"""
MAPE 10-20% é”æˆã®ãŸã‚ã®è¶…ä¿å®ˆçš„ç¾å®Ÿã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
æ ¹æœ¬æ–¹é‡ï¼šå°ã•ãç¢ºå®Ÿãªäºˆæ¸¬ã§é«˜ç²¾åº¦ã‚’å®Ÿç¾
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional
import logging
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import Ridge
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import TimeSeriesSplit
import warnings
warnings.filterwarnings('ignore')

from data.stock_data import StockDataProvider

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ConservativeMAPEAchiever:
    """ä¿å®ˆçš„ã§ç¾å®Ÿçš„ãªMAPE 10-20%é”æˆã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        self.data_provider = StockDataProvider()
        self.scaler = StandardScaler()

    def create_stable_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """å®‰å®šã—ãŸäºˆæ¸¬å¯èƒ½ãªç‰¹å¾´é‡ã®ã¿ä½œæˆ"""
        features = pd.DataFrame(index=data.index)

        close = data['Close']
        volume = data['Volume']
        high = data['High']
        low = data['Low']

        # 1. åŸºæœ¬ç§»å‹•å¹³å‡ç³»ï¼ˆæœ€ã‚‚ä¿¡é ¼æ€§ãŒé«˜ã„ï¼‰
        sma_5 = close.rolling(5).mean()
        sma_10 = close.rolling(10).mean()
        sma_20 = close.rolling(20).mean()

        # ä¾¡æ ¼ã¨ç§»å‹•å¹³å‡ã®é–¢ä¿‚ï¼ˆæ­£è¦åŒ–æ¸ˆã¿ï¼‰
        features['price_sma5_ratio'] = (close - sma_5) / sma_5
        features['price_sma20_ratio'] = (close - sma_20) / sma_20
        features['sma5_sma20_ratio'] = (sma_5 - sma_20) / sma_20

        # 2. ã‚·ãƒ³ãƒ—ãƒ«ãªãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆå®‰å®šï¼‰
        returns = close.pct_change()
        features['volatility_5'] = returns.rolling(5).std()
        features['volatility_20'] = returns.rolling(20).std()

        # 3. RSIï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰
        rsi_14 = self._calculate_simple_rsi(close, 14)
        features['rsi_14'] = rsi_14
        features['rsi_oversold'] = (rsi_14 < 30).astype(int)
        features['rsi_overbought'] = (rsi_14 > 70).astype(int)

        # 4. ä¾¡æ ¼ä½ç½®ï¼ˆãƒ¬ãƒ³ã‚¸å†…ï¼‰
        max_20 = high.rolling(20).max()
        min_20 = low.rolling(20).min()
        features['price_position'] = (close - min_20) / (max_20 - min_20)

        # 5. å‡ºæ¥é«˜æ¯”ç‡ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ï¼‰
        volume_avg = volume.rolling(20).mean()
        features['volume_ratio'] = volume / volume_avg

        # 6. ç›´è¿‘ãƒªã‚¿ãƒ¼ãƒ³ï¼ˆé™å®šçš„ï¼‰
        features['return_1d'] = returns
        features['return_3d'] = close.pct_change(3)

        # 7. ãƒˆãƒ¬ãƒ³ãƒ‰æ–¹å‘ï¼ˆåˆ†é¡çš„ï¼‰
        features['trend_up'] = (sma_5 > sma_20).astype(int)
        features['trend_strength'] = np.abs(features['sma5_sma20_ratio'])

        return features

    def _calculate_simple_rsi(self, prices: pd.Series, window: int = 14) -> pd.Series:
        """ã‚·ãƒ³ãƒ—ãƒ«ãªRSIè¨ˆç®—"""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
        rs = gain / loss
        return 100 - (100 / (1 + rs))

    def create_conservative_target(self, data: pd.DataFrame, prediction_days: int = 3) -> pd.Series:
        """ä¿å®ˆçš„ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ï¼ˆçŸ­æœŸäºˆæ¸¬ï¼‰"""
        close = data['Close']

        # çŸ­æœŸã®å¹³å‡ãƒªã‚¿ãƒ¼ãƒ³ï¼ˆã‚ˆã‚Šäºˆæ¸¬ã—ã‚„ã™ã„ï¼‰
        future_return = close.shift(-prediction_days).pct_change(prediction_days)

        return future_return

    def filter_predictable_samples(self, features: pd.DataFrame, target: pd.Series) -> Tuple[pd.DataFrame, pd.Series]:
        """äºˆæ¸¬å¯èƒ½ãªã‚µãƒ³ãƒ—ãƒ«ã®ã¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        # 1. åŸºæœ¬çš„ãªæ¸…æƒ
        features_clean = features.fillna(method='ffill').fillna(0)
        features_clean = features_clean.replace([np.inf, -np.inf], 0)

        # 2. æœ‰åŠ¹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        valid_idx = ~(features_clean.isna().any(axis=1) | target.isna())

        # 3. æ¥µç«¯ãªå€¤ã®é™¤å¤–ï¼ˆäºˆæ¸¬å›°é›£ï¼‰
        target_clean = target[valid_idx]

        # æ¥µç«¯ãªãƒªã‚¿ãƒ¼ãƒ³ã‚’é™¤å¤–ï¼ˆÂ±10%ä»¥ä¸Šï¼‰
        extreme_mask = np.abs(target_clean) < 0.1

        features_filtered = features_clean[valid_idx][extreme_mask]
        target_filtered = target_clean[extreme_mask]

        # 4. ã•ã‚‰ã«å®‰å®šã—ãŸæœŸé–“ã®ã¿é¸æŠ
        volatility = np.abs(target_filtered).rolling(10).std()
        stable_mask = volatility < volatility.median()

        return features_filtered[stable_mask], target_filtered[stable_mask]

    def train_conservative_model(self, X: pd.DataFrame, y: pd.Series) -> Tuple[object, float]:
        """ä¿å®ˆçš„ãƒ¢ãƒ‡ãƒ«è¨“ç·´"""
        # ã‚·ãƒ³ãƒ—ãƒ«ã§å®‰å®šã—ãŸãƒ¢ãƒ‡ãƒ«ã®ã¿ä½¿ç”¨
        models = {
            'ridge': Ridge(alpha=1.0),
            'random_forest': RandomForestRegressor(
                n_estimators=50,
                max_depth=5,
                min_samples_split=20,
                min_samples_leaf=10,
                random_state=42
            )
        }

        best_model = None
        best_mape = float('inf')

        # æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        tscv = TimeSeriesSplit(n_splits=3)

        for name, model in models.items():
            mape_scores = []

            for train_idx, val_idx in tscv.split(X):
                X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
                y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

                # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
                X_train_scaled = self.scaler.fit_transform(X_train)
                X_val_scaled = self.scaler.transform(X_val)

                # è¨“ç·´
                model.fit(X_train_scaled, y_train)

                # äºˆæ¸¬
                y_pred = model.predict(X_val_scaled)

                # MAPEè¨ˆç®—ï¼ˆå³æ ¼ç‰ˆï¼‰
                mape = self.calculate_strict_mape(y_val, y_pred)
                if not np.isfinite(mape):
                    mape = 999.0

                mape_scores.append(mape)

            avg_mape = np.mean(mape_scores)
            print(f"  {name}: MAPE {avg_mape:.2f}%")

            if avg_mape < best_mape:
                best_mape = avg_mape
                best_model = model

        # æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã‚’å…¨ãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´
        X_scaled = self.scaler.fit_transform(X)
        best_model.fit(X_scaled, y)

        return best_model, best_mape

    def calculate_strict_mape(self, actual: pd.Series, predicted: pd.Series, min_threshold: float = 0.01) -> float:
        """å³æ ¼ãªMAPEè¨ˆç®—ï¼ˆå¤§ããªå‹•ãã®ã¿è©•ä¾¡ï¼‰"""
        actual_arr = np.array(actual)
        predicted_arr = np.array(predicted)

        # ã‚ˆã‚Šå¤§ããªé–¾å€¤ï¼ˆ1%ä»¥ä¸Šã®å‹•ãã®ã¿ï¼‰
        mask = np.abs(actual_arr) >= min_threshold

        if mask.sum() < 3:
            return float('inf')

        filtered_actual = actual_arr[mask]
        filtered_predicted = predicted_arr[mask]

        # ç•°å¸¸å€¤ã®é™¤å¤–
        error_ratios = np.abs((filtered_actual - filtered_predicted) / filtered_actual)
        valid_errors = error_ratios[error_ratios < 2.0]  # 200%ä»¥ä¸‹ã®ã‚¨ãƒ©ãƒ¼ã®ã¿

        if len(valid_errors) < 3:
            return float('inf')

        return np.mean(valid_errors) * 100

    def test_conservative_system(self, symbols: List[str]) -> Dict:
        """ä¿å®ˆçš„ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆ"""
        print("\nä¿å®ˆçš„MAPE 10-20%é”æˆã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ")
        print("=" * 60)

        all_results = []
        success_count = 0

        for symbol in symbols[:15]:  # ã‚ˆã‚Šå¤šãã®éŠ˜æŸ„ã§ãƒ†ã‚¹ãƒˆ
            try:
                print(f"\nå‡¦ç†ä¸­: {symbol}")

                # ãƒ‡ãƒ¼ã‚¿å–å¾—
                data = self.data_provider.get_stock_data(symbol, "1y")
                if len(data) < 100:
                    continue

                # ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆä½œæˆ
                features = self.create_stable_features(data)
                target = self.create_conservative_target(data, prediction_days=3)

                # äºˆæ¸¬å¯èƒ½ã‚µãƒ³ãƒ—ãƒ«ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                X_filtered, y_filtered = self.filter_predictable_samples(features, target)

                if len(X_filtered) < 30:
                    print(f"  ã‚¹ã‚­ãƒƒãƒ—: äºˆæ¸¬å¯èƒ½ã‚µãƒ³ãƒ—ãƒ«ä¸è¶³ ({len(X_filtered)})")
                    continue

                print(f"  äºˆæ¸¬å¯èƒ½ã‚µãƒ³ãƒ—ãƒ«: {len(X_filtered)}")

                # è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆåˆ†å‰²ï¼ˆæœ€æ–°30%ã‚’ãƒ†ã‚¹ãƒˆï¼‰
                split_point = int(len(X_filtered) * 0.7)
                X_train = X_filtered.iloc[:split_point]
                y_train = y_filtered.iloc[:split_point]
                X_test = X_filtered.iloc[split_point:]
                y_test = y_filtered.iloc[split_point:]

                if len(X_test) < 10:
                    continue

                # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
                model, train_mape = self.train_conservative_model(X_train, y_train)

                # ãƒ†ã‚¹ãƒˆäºˆæ¸¬
                X_test_scaled = self.scaler.transform(X_test)
                test_predictions = model.predict(X_test_scaled)

                # ãƒ†ã‚¹ãƒˆMAPE
                test_mape = self.calculate_strict_mape(y_test, test_predictions)

                result = {
                    'symbol': symbol,
                    'train_mape': train_mape,
                    'test_mape': test_mape,
                    'test_samples': len(X_test),
                    'predictable_samples': len(X_filtered)
                }

                all_results.append(result)

                print(f"  è¨“ç·´MAPE: {train_mape:.2f}%")
                print(f"  ãƒ†ã‚¹ãƒˆMAPE: {test_mape:.2f}%")

                if test_mape <= 20:
                    success_count += 1
                    print("  âœ“ ç›®æ¨™é”æˆï¼")
                elif test_mape <= 30:
                    print("  â–³ è‰¯å¥½")

            except Exception as e:
                logger.warning(f"Error processing {symbol}: {str(e)}")
                continue

        # çµæœåˆ†æ
        if all_results:
            valid_results = [r for r in all_results if np.isfinite(r['test_mape'])]

            if valid_results:
                test_mapes = [r['test_mape'] for r in valid_results]
                median_mape = np.median(test_mapes)
                mean_mape = np.mean(test_mapes)

                print(f"\n" + "=" * 60)
                print("æœ€çµ‚çµæœ")
                print("=" * 60)
                print(f"æœ‰åŠ¹éŠ˜æŸ„æ•°: {len(valid_results)}")
                print(f"ä¸­å¤®å€¤MAPE: {median_mape:.2f}%")
                print(f"å¹³å‡MAPE: {mean_mape:.2f}%")
                print(f"æˆåŠŸéŠ˜æŸ„ (â‰¤20%): {success_count}/{len(valid_results)}")
                print(f"æˆåŠŸç‡: {success_count/len(valid_results)*100:.1f}%")

                # æˆåŠŸä¾‹ã®è¡¨ç¤º
                successful = [r for r in valid_results if r['test_mape'] <= 20]
                if successful:
                    print(f"\næˆåŠŸéŠ˜æŸ„è©³ç´°:")
                    for r in successful:
                        print(f"  {r['symbol']}: {r['test_mape']:.2f}%")

                if median_mape <= 20:
                    print(f"\nğŸ‰ ç›®æ¨™é”æˆï¼ä¸­å¤®å€¤MAPE {median_mape:.2f}%")
                    return {'success': True, 'median_mape': median_mape, 'results': valid_results}
                else:
                    print(f"\nâ–³ æ”¹å–„ä¸­ï¼šç›®æ¨™ã¾ã§æ®‹ã‚Š{median_mape - 20:.1f}%")
                    return {'success': False, 'median_mape': median_mape, 'results': valid_results}

        return {'error': 'No valid results'}

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("ä¿å®ˆçš„MAPE 10-20%é”æˆã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 60)

    data_provider = StockDataProvider()
    symbols = list(data_provider.jp_stock_codes.keys())

    achiever = ConservativeMAPEAchiever()
    results = achiever.test_conservative_system(symbols)

    if 'error' not in results:
        if results.get('success'):
            print(f"\nâœ“ ChatGPTç†è«–å®Ÿè¨¼ï¼šMAPE {results['median_mape']:.2f}%ã§ç›®æ¨™é”æˆï¼")
        else:
            print(f"\nç¶™ç¶šæ”¹å–„ï¼šç¾åœ¨{results['median_mape']:.2f}%")

if __name__ == "__main__":
    main()